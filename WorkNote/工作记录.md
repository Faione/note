# collector源码

# 联调大车

# CLI开发

# 心跳状态检查


流程修改
   - 控制中心只管理 启动 与 关闭
      - 其他逻辑由策略完成
   - agent 进行过滤
      - 防止信息过多、
   - 集成前端ui，query到策略中

Agent设计
   - agent 监听多个topic 
     - 多个topic的生成、销毁对性能的影响
   - 系统组件心跳 
   - 轮询观察状态

流程扩充
   - 集群资源监控


metric数据结构定义
   - container
   - host
   - map进行可扩展

控制器与收集器定义
   - 提供基本函数

etcd持久化

jaeger-collector测试
   - 简单api接收器
   - 连接 agent -> collector


连接存在的问题
   - grpc验证问题，collector拒绝了agent的gpc请求，导致agent无法将span数据发送给collector

报错
```json
 {
    "level": "error",
    "ts": 1636541439.3878434,
    "caller": "grpc/reporter.go:74",
    "msg": "Could not send spans over gRPC",
    "error": "rpc error: code = Unavailable desc = last connection error:connection error: desc = \"transport: Error while dialing dial tcp 172.30.0.2:14250: connect: connection refused\"",
    "stacktrace": "github.com/jaegertracing/jaeger/cmd/agent/app/reporter/grpc.(*Reporter).send\n\t/go/src/github.com/jaegertracing/jaeger/cmd/agent/app/reporter/grpc/reporter.go:74\ngithub.com/jaegertracing/jaeger/cmd/agent/app/reporter/grpc.(*Reporter).EmitBatch\n\t/go/src/github.com/jaegertracing/jaeger/cmd/agent/app/reporter/grpc/reporter.go:53\ngithub.com/jaegertracing/jaeger/cmd/agent/app/reporter.(*MetricsReporter).EmitBatch\n\t/go/src/github.com/jaegertracing/jaeger/cmd/agent/app/reporter/metrics.go:85\ngithub.com/jaegertracing/jaeger/cmd/agent/app/reporter.(*ClientMetricsReporter).EmitBatch\n\t/go/src/github.com/jaegertracing/jaeger/cmd/agent/app/reporter/client_metrics.go:121\ngithub.com/jaegertracing/jaeger/thrift-gen/agent.(*agentProcessorEmitBatch).Process\n\t/go/src/github.com/jaegertracing/jaeger/thrift-gen/agent/agent.go:176\ngithub.com/jaegertracing/jaeger/thrift-gen/agent.(*AgentProcessor).Process\n\t/go/src/github.com/jaegertracing/jaeger/thrift-gen/agent/agent.go:123\ngithub.com/jaegertracing/jaeger/cmd/agent/app/processors.(*ThriftProcessor).processBuffer\n\t/go/src/github.com/jaegertracing/jaeger/cmd/agent/app/processors/thrift_processor.go:122\ngithub.com/jaegertracing/jaeger/cmd/agent/app/processors.NewThriftProcessor.func2\n\t/go/src/github.com/jaegertracing/jaeger/cmd/agent/app/processors/thrift_processor.go:87"
}
```
- 思路：证书问题
   - 按照 jaeger 官方提供的DockerFile构造 base_image
   - 编译 agent、collector，通过 base_image构建镜像
- 结果
   - 仍然存在验证问题
- 计划
   - 查看grpc机制  


define里面是agent提供的基础方法

注册
创建收集器

插件的功能去pkg中增加
   编解码span 数据（model.span）

对容器进行监控、控制（应用无感）
Host进行监控

agent启动容器
   - 提供用户模板文件，用户填写并提交给agent
   - 获得注册信息，注册guid
   - guid作为环境变量，放入容器


容器guid: 镜像名称 + 随机数

collect -> manager



jaeger collector 增加 container id 筛选
   - get中，做span数据的解析与分类
   - 提供对于单个通道的关闭

guid 与 trace数据绑定 ： 使用 guid 作为 service name
1. tracer构建时传入 service name 标识当前追踪的进程
   - examples\hotrod\cmd\frontend.go
2. 打包span时， span.Process 保存了 service name 这一数据
   - model\model.pb.go
3. 解析 span 数据， 取出 Process 字段，就能够得到 service name




创建 jaeger-env 容器
   - 应用与 jaeger-env 交互
      - jaeger-env 暴露固定端口
         - StartContainer提供参数 ports
      - 应用与 jaeger-env 使用同一网络   


agent plugin http writer写数据时
  - host:port/get/{guid}
agent 接收到span数据的时
  - 维护 map[guid] chan []byte




log信息缓冲，未消费的话，维护一个队列


Collector

1124
   - manager - agent 联调测试



dcoker container -> cgroup path

net: 发送速率


- agent提供 k8s、opent stack 



jaeger UI
   - search
      - 左边 
   - trace detail
      - 核心 ms 
   - DAG: 整体数据(次数、延迟)
      - 选好应用之后


docker 私有库搭建
 

测试一

   - 初始化
      - 绑定服务容器与干扰容器到同一核心
         - 脚本绑定   
      - CPU核心分配

   - 服务容器
      - CPU使用率正弦波形
 
   - CPU干扰容器
      - 始终占满给定的CPU资源

   - 实验场景
      - 1、服务容器正常使用时正弦
      - 2、无调控，服务容器、干扰容器同时开启，服务容器使用率平缓且较低，资源无法满足
      - 3、有调控, 服务容器、干扰容器同时开启，服务容器使用率正常，资源满足


1. 小车
 - 服务容器
   - CPU使用率正弦波形
 - CPU干扰容器
      - 始终占满给定的CPU资源

2. 初始化
      - 绑定服务容器与干扰容器到同一核心
         - 脚本绑定   
      - CPU核心分配

3. 服务容器正常使用时正弦

4. 无调控，服务容器、干扰容器同时开启，服务容器使用率平缓且较低，资源无法满足

5. 有调控, 服务容器、干扰容器同时开启，服务容器使用率正常，资源满足


agent接入增加版本信息


1207
   - 从span数据中计算端到端延迟
   - agent关闭逻辑，是否需要注销所有对象
   - 针对 centos 做 cgroup
   - 测试 cpu 核心调整
   - 完善 agent 退出
   - 处理 no such function guid


1209
web讨论
- 前端
- 启动 agent
- 镜像存储, Docker私有库


agent 注册时增加 tag, 区别自己与其他应用
agent 保持发送心跳, 更新自己状态

sponge 提供 list 查询

搭建 jaeger-query


前端
   - 完善查询界面
   - 增加界面说明, 关联trace与dag
   - 根据请求数量区分dag节点
   - 心跳信息中增加动态指标信息

使用教程
   - 安装、测试、Q&A

基本规范

解决 vscode remote ssh 错误
   - 无法建立 code server 问题
      - 端口转发情况下
调研linux用户组管理  


固化常用tag

调研openstack

HotROD
   - 描述: UI，四个微服务，两个存储后端 
   - frontend: 
      - 描述: 前端微服务为 Javascript UI 提供服务，并对其他三个微服务进行 RPC 调用
         - 用户点击4个地址中的任意一个, 向后端发送搭车请求
         - 后端向前端返回汽车的车牌号和预计到达的时间作为响应 
   - customer
      - 描述: 接收 frontend 的 get 请求, 并执行一次数据库查询, 检查当前用户
   - driver   
      - 描述: 接收 frontend 的 RPC 请求, 并执行一系列Redis请求, 请求存在失败的可能, 找到司机(可能多个)
   - route:
      - 描述: frontend 根据上述请求的结果, 向 route 发送 get 请求,  route 返回路径信息给 frontend, frontend 处理后, 返回给用户

使用GPU的容器问题
   - 动态链接库失效, 同节点上镜像失效
   - agent对用户异构需求的支持
      - 主流之外，还有FPGA, npu等

测试 demo 启动顺序
   - darknet 
   - camera catcher
   - ui(启动websocket, 消费图片)

端到端追踪页面

agent控制k8s

tag 图可视化
   - 分级tag


agent 注册打tag
   - 定义请求结构体
   - 定义返回结构体
   - 更新 rpc
   - 更新 create 方法
   - 更新返回

agent 支持 docker-compose 

标签区分所有guid区分, 标注不运行自动删除的对象
   - auto delete

span多层解析

分页，内容描述

auto refresh


sponge && agent 代码review




viper
读比特流, 解析并存到文件里


注册传入 []请求
成功: map[applicationkey] reponse
失败: nil

application key 作为 key携带返回结果
   启动成功携带各个结果，否则全部删除
   假定操作

注册完成之后不用 ready 状态 (pendding)
   inspect 之后修改为 ready
 
提供 stop 与 restart

定义状态变化: terminating ->


RPC方法重命名


启动应用跳转的连接
   - web url 提取

小车demo优化(推迟)
完善小车应用的展示需求
   - 小车自己的延迟

小车页面点击之后可以看到延迟, 策略执行

单独追踪某一帧
   图片抓取服务闭环
      图片抓取程序最终收到前端发送的span

云服务 trace 接入规约

架构图更新, agent 提供 jaeger env
   - jaeger client 与 agent 绑定
      - metric、log、trace -> ES



frp安全性
   - tls, srpc

agent 抓udp
   - 收取 span
   - sampling

redis json

agent功能优化
   - handler中进行超时处理


dependency + dag

traces

trace

返回 data + err

直接完成方法


nnmsg R&Q 模型
   - https://github.com/codypiersall/pynng/blob/master/examples/reqprep.py


实时变换的延迟曲线
帧数率

agent 适配yml文件

containers:
  agent:
    appkey: trace_jaeger_client
    parent: trace_jaeger_collector
    child: ""
    name: ""
    image: jaeger-env
    cid: ""
    guid: ""
    status: NotReady
    portmap:
      "80": "8080"
    envskey:
      SUPB_GUID: "123456"
    cmd:
    - /bin/sh
    - boot.sh
    - hello
  collector:
    appkey: trace_jaeger_collector
    parent: supb_agent
    child: trace_jaeger_client
    name: ""
    image: jaeger-collector
    cid: ""
    guid: ""
    status: NotReady
    portmap:
      "80": "8080"
    envskey:
      SUPB_GUID: "123456"
    cmd:
    - /bin/sh
    - boot.sh
    - hello


模拟服务应用、模拟干扰应用
小车服务、图像处理服务


span 与 trace 分开
   - active trace bug
性能影响
   - Default rate: 10
   - Normal rate: 8-9
   - Exp rate: 6-7


frontend
   使用返回的端口，不同的地址连

camera service 
  -> dark 返回一个端口


todo 使用同一个 sock

benchmark容器


containers:
  emu-service:
    appkey: "one-core-emu-service"
    parent: ""
    child: ""
    name: ""
    image: 39.101.140.145:5000/cpu-ctrl-core-1
    cid: ""
    guid: ""
    status: NotReady
    portmap:
    envskey:
      SUPB_GUID: ""
    cmd:
    - /bin/sh
    - boot.sh

containers:
  cpu-interference-service:
    appkey: "one-core-interference-service"
    parent: ""
    child: ""
    name: ""
    image: 39.101.140.145:5000/interference-cpu-core-1
    cid: ""
    guid: ""
    status: NotReady
    portmap:
    envskey:
      SUPB_GUID: ""
    cmd:
    - /bin/sh
    - boot.sh


ip在环境变量
生成tag: host + port 

电调
方向舵机
摄像头舵机-上
摄像头舵机-下




1] sync info: %v&{map[01ad28f000:2 01d3dba500:2] map[01ad28f000:{0-95 1} 01d3dba500:{0-95 1}]}  function=agent.heartbeat


空包打时间戳
- 探测延迟

发送图片时间戳差异大
接收图片时间戳差异不大
  - 调整buffer 



- 子系统代码量，语言
  - 代码量: -
  - 语言: go  
- 子系统功能列表

|功能||说明|
|:---|:---|:---|
|调控功能|容器级别|基于docker api的容器生命周期管理|
|||基于cgroup的容器CPU、内存、I/O资源管理|
|||基于tc工具的上行网络带宽、延迟控制|
|监控功能|Host级别|Host主机的CPU、内存、网络、I/O资源监控|
||容器级别|基于docker api的容器CPU、内存、网络、I/O资源监控|
|||基于docker api的容器运行log监控|
||应用级别|基于opentracing/jaeger的链路延迟监控|
|系统功能|状态管理|容器运行状态的心跳监控|
||RPC|基于总线的远程调控方法调用|
||接入/退出|动态地控制主机、容器等再系统中的接入/退出|

边缘服务器

CICD
形式化验证

agent-2021
- 信息高铁测调节点代理研究，实现调控数据、日志、延迟时间等跨区域、跨平台、跨设备的执行与采集。
agent-2022
- 信息高铁测调节点代理迭代，控制上实现OpenStack虚拟机、K8s容器编排系统的接入, 监控上对资源、日志、追踪的数据格式进行统一化方便存储与检索, 同时优化节点代理, 增强鲁棒性、稳定性, 降低资源开销, 实现性能的提升
- 信息高铁测调节点代理迭代, 实现对虚机、集群的控制，着力端到端追踪功能完善，并对其他指标进行标准化，优化节点代理，增强稳定性，提高性能
- 技术路线
  - 节点代理已有功能能够对容器、节点进行的监控与控制，但为契合信息高铁的发展以及当前云计算技术发展环境，节点代理应当提供对于虚机及集群的控制能力，来增加调控的维度，参考cncf目录，目前可行的技术有openstack，kubernetes与kubevirt，其分别对应于虚拟机平台，集群容器编排与尚未容器化的虚拟机编排，而在监控上，当前基于jaeger的链路追踪系统仍有不足之处，而作为接替Opentelemetry则能够提供更长久的技术支持与功能覆盖
    - OpenStack始于2010年，是Rackspace和美国国家航空航天局的合作项目，它主要作为基础设施即服务（IaaS）部署在公用云和私有云中，提供虚拟服务器和其他资源给用户使用。OpenStack平台由相互关联的组件组成，控制着整个数据中心内不同的厂商的处理器、存储和网络资源的硬件池。用户可以通过基于网络的仪表盘、命令行工具或RESTful网络服务来管理。
    - KubeVirt 是 Kubernetes 的虚拟机管理插件。目的是为 Kubernetes 之上的虚拟化解决方案提供一个共同基础。KubeVirt 的核心是通过 Kubernetes 的自定义资源定义 API 添加额外的虚拟化资源类型（尤其是 VM 类型）来扩展 Kubernetes。通过使用这种机制，Kubernetes API 可用于管理这些 VM 资源以及 Kubernetes 提供的所有其他资源。
    - OpenTelemetry 是一组 API、SDK、工具和集成，旨在创建和管理遥测数据，例如跟踪、指标和日志。该项目提供了一个与供应商无关的实现，可以将其配置为将遥测数据发送到用户选择的后端
  - 本项目着力节点代理的优化与功能强化，为顶层设计提供更丰富的监控维度与调控方式，满足当前及未来的需求


每台node上都安装agent
agent相互通信
Pod网络

opentelemtry 和 opentracing 和 jaeger client的关系














# collector源码

# 联调大车

# CLI开发

# 心跳状态检查


流程修改
   - 控制中心只管理 启动 与 关闭
      - 其他逻辑由策略完成
   - agent 进行过滤
      - 防止信息过多、
   - 集成前端ui，query到策略中

Agent设计
   - agent 监听多个topic 
     - 多个topic的生成、销毁对性能的影响
   - 系统组件心跳 
   - 轮询观察状态

流程扩充
   - 集群资源监控


metric数据结构定义
   - container
   - host
   - map进行可扩展

控制器与收集器定义
   - 提供基本函数

etcd持久化

jaeger-collector测试
   - 简单api接收器
   - 连接 agent -> collector


连接存在的问题
   - grpc验证问题，collector拒绝了agent的gpc请求，导致agent无法将span数据发送给collector

报错
```json
 {
    "level": "error",
    "ts": 1636541439.3878434,
    "caller": "grpc/reporter.go:74",
    "msg": "Could not send spans over gRPC",
    "error": "rpc error: code = Unavailable desc = last connection error:connection error: desc = \"transport: Error while dialing dial tcp 172.30.0.2:14250: connect: connection refused\"",
    "stacktrace": "github.com/jaegertracing/jaeger/cmd/agent/app/reporter/grpc.(*Reporter).send\n\t/go/src/github.com/jaegertracing/jaeger/cmd/agent/app/reporter/grpc/reporter.go:74\ngithub.com/jaegertracing/jaeger/cmd/agent/app/reporter/grpc.(*Reporter).EmitBatch\n\t/go/src/github.com/jaegertracing/jaeger/cmd/agent/app/reporter/grpc/reporter.go:53\ngithub.com/jaegertracing/jaeger/cmd/agent/app/reporter.(*MetricsReporter).EmitBatch\n\t/go/src/github.com/jaegertracing/jaeger/cmd/agent/app/reporter/metrics.go:85\ngithub.com/jaegertracing/jaeger/cmd/agent/app/reporter.(*ClientMetricsReporter).EmitBatch\n\t/go/src/github.com/jaegertracing/jaeger/cmd/agent/app/reporter/client_metrics.go:121\ngithub.com/jaegertracing/jaeger/thrift-gen/agent.(*agentProcessorEmitBatch).Process\n\t/go/src/github.com/jaegertracing/jaeger/thrift-gen/agent/agent.go:176\ngithub.com/jaegertracing/jaeger/thrift-gen/agent.(*AgentProcessor).Process\n\t/go/src/github.com/jaegertracing/jaeger/thrift-gen/agent/agent.go:123\ngithub.com/jaegertracing/jaeger/cmd/agent/app/processors.(*ThriftProcessor).processBuffer\n\t/go/src/github.com/jaegertracing/jaeger/cmd/agent/app/processors/thrift_processor.go:122\ngithub.com/jaegertracing/jaeger/cmd/agent/app/processors.NewThriftProcessor.func2\n\t/go/src/github.com/jaegertracing/jaeger/cmd/agent/app/processors/thrift_processor.go:87"
}
```
- 思路：证书问题
   - 按照 jaeger 官方提供的DockerFile构造 base_image
   - 编译 agent、collector，通过 base_image构建镜像
- 结果
   - 仍然存在验证问题
- 计划
   - 查看grpc机制  


define里面是agent提供的基础方法

注册
创建收集器

插件的功能去pkg中增加
   编解码span 数据（model.span）

对容器进行监控、控制（应用无感）
Host进行监控

agent启动容器
   - 提供用户模板文件，用户填写并提交给agent
   - 获得注册信息，注册guid
   - guid作为环境变量，放入容器


容器guid: 镜像名称 + 随机数

collect -> manager



jaeger collector 增加 container id 筛选
   - get中，做span数据的解析与分类
   - 提供对于单个通道的关闭

guid 与 trace数据绑定 ： 使用 guid 作为 service name
1. tracer构建时传入 service name 标识当前追踪的进程
   - examples\hotrod\cmd\frontend.go
2. 打包span时， span.Process 保存了 service name 这一数据
   - model\model.pb.go
3. 解析 span 数据， 取出 Process 字段，就能够得到 service name




创建 jaeger-env 容器
   - 应用与 jaeger-env 交互
      - jaeger-env 暴露固定端口
         - StartContainer提供参数 ports
      - 应用与 jaeger-env 使用同一网络   


agent plugin http writer写数据时
  - host:port/get/{guid}
agent 接收到span数据的时
  - 维护 map[guid] chan []byte




log信息缓冲，未消费的话，维护一个队列


Collector

1124
   - manager - agent 联调测试



dcoker container -> cgroup path

net: 发送速率


- agent提供 k8s、opent stack 



jaeger UI
   - search
      - 左边 
   - trace detail
      - 核心 ms 
   - DAG: 整体数据(次数、延迟)
      - 选好应用之后


docker 私有库搭建
 

测试一

   - 初始化
      - 绑定服务容器与干扰容器到同一核心
         - 脚本绑定   
      - CPU核心分配

   - 服务容器
      - CPU使用率正弦波形
 
   - CPU干扰容器
      - 始终占满给定的CPU资源

   - 实验场景
      - 1、服务容器正常使用时正弦
      - 2、无调控，服务容器、干扰容器同时开启，服务容器使用率平缓且较低，资源无法满足
      - 3、有调控, 服务容器、干扰容器同时开启，服务容器使用率正常，资源满足


1. 小车
 - 服务容器
   - CPU使用率正弦波形
 - CPU干扰容器
      - 始终占满给定的CPU资源

2. 初始化
      - 绑定服务容器与干扰容器到同一核心
         - 脚本绑定   
      - CPU核心分配

3. 服务容器正常使用时正弦

4. 无调控，服务容器、干扰容器同时开启，服务容器使用率平缓且较低，资源无法满足

5. 有调控, 服务容器、干扰容器同时开启，服务容器使用率正常，资源满足


agent接入增加版本信息


1207
   - 从span数据中计算端到端延迟
   - agent关闭逻辑，是否需要注销所有对象
   - 针对 centos 做 cgroup
   - 测试 cpu 核心调整
   - 完善 agent 退出
   - 处理 no such function guid


1209
web讨论
- 前端
- 启动 agent
- 镜像存储, Docker私有库


agent 注册时增加 tag, 区别自己与其他应用
agent 保持发送心跳, 更新自己状态

sponge 提供 list 查询

搭建 jaeger-query


前端
   - 完善查询界面
   - 增加界面说明, 关联trace与dag
   - 根据请求数量区分dag节点
   - 心跳信息中增加动态指标信息

使用教程
   - 安装、测试、Q&A

基本规范

解决 vscode remote ssh 错误
   - 无法建立 code server 问题
      - 端口转发情况下
调研linux用户组管理  


固化常用tag

调研openstack

HotROD
   - 描述: UI，四个微服务，两个存储后端 
   - frontend: 
      - 描述: 前端微服务为 Javascript UI 提供服务，并对其他三个微服务进行 RPC 调用
         - 用户点击4个地址中的任意一个, 向后端发送搭车请求
         - 后端向前端返回汽车的车牌号和预计到达的时间作为响应 
   - customer
      - 描述: 接收 frontend 的 get 请求, 并执行一次数据库查询, 检查当前用户
   - driver   
      - 描述: 接收 frontend 的 RPC 请求, 并执行一系列Redis请求, 请求存在失败的可能, 找到司机(可能多个)
   - route:
      - 描述: frontend 根据上述请求的结果, 向 route 发送 get 请求,  route 返回路径信息给 frontend, frontend 处理后, 返回给用户

使用GPU的容器问题
   - 动态链接库失效, 同节点上镜像失效
   - agent对用户异构需求的支持
      - 主流之外，还有FPGA, npu等

测试 demo 启动顺序
   - darknet 
   - camera catcher
   - ui(启动websocket, 消费图片)

端到端追踪页面

agent控制k8s

tag 图可视化
   - 分级tag


agent 注册打tag
   - 定义请求结构体
   - 定义返回结构体
   - 更新 rpc
   - 更新 create 方法
   - 更新返回

agent 支持 docker-compose 

标签区分所有guid区分, 标注不运行自动删除的对象
   - auto delete

span多层解析

分页，内容描述

auto refresh


sponge && agent 代码review




viper
读比特流, 解析并存到文件里


注册传入 []请求
成功: map[applicationkey] reponse
失败: nil

application key 作为 key携带返回结果
   启动成功携带各个结果，否则全部删除
   假定操作

注册完成之后不用 ready 状态 (pendding)
   inspect 之后修改为 ready
 
提供 stop 与 restart

定义状态变化: terminating ->


RPC方法重命名


启动应用跳转的连接
   - web url 提取

小车demo优化(推迟)
完善小车应用的展示需求
   - 小车自己的延迟

小车页面点击之后可以看到延迟, 策略执行

单独追踪某一帧
   图片抓取服务闭环
      图片抓取程序最终收到前端发送的span

云服务 trace 接入规约

架构图更新, agent 提供 jaeger env
   - jaeger client 与 agent 绑定
      - metric、log、trace -> ES



frp安全性
   - tls, srpc

agent 抓udp
   - 收取 span
   - sampling

redis json

agent功能优化
   - handler中进行超时处理


dependency + dag

traces

trace

返回 data + err

直接完成方法


nnmsg R&Q 模型
   - https://github.com/codypiersall/pynng/blob/master/examples/reqprep.py


实时变换的延迟曲线
帧数率

agent 适配yml文件

containers:
  agent:
    appkey: trace_jaeger_client
    parent: trace_jaeger_collector
    child: ""
    name: ""
    image: jaeger-env
    cid: ""
    guid: ""
    status: NotReady
    portmap:
      "80": "8080"
    envskey:
      SUPB_GUID: "123456"
    cmd:
    - /bin/sh
    - boot.sh
    - hello
  collector:
    appkey: trace_jaeger_collector
    parent: supb_agent
    child: trace_jaeger_client
    name: ""
    image: jaeger-collector
    cid: ""
    guid: ""
    status: NotReady
    portmap:
      "80": "8080"
    envskey:
      SUPB_GUID: "123456"
    cmd:
    - /bin/sh
    - boot.sh
    - hello


模拟服务应用、模拟干扰应用
小车服务、图像处理服务


span 与 trace 分开
   - active trace bug
性能影响
   - Default rate: 10
   - Normal rate: 8-9
   - Exp rate: 6-7


frontend
   使用返回的端口，不同的地址连

camera service 
  -> dark 返回一个端口


todo 使用同一个 sock

benchmark容器


containers:
  emu-service:
    appkey: "one-core-emu-service"
    parent: ""
    child: ""
    name: ""
    image: 39.101.140.145:5000/cpu-ctrl-core-1
    cid: ""
    guid: ""
    status: NotReady
    portmap:
    envskey:
      SUPB_GUID: ""
    cmd:
    - /bin/sh
    - boot.sh

containers:
  cpu-interference-service:
    appkey: "one-core-interference-service"
    parent: ""
    child: ""
    name: ""
    image: 39.101.140.145:5000/interference-cpu-core-1
    cid: ""
    guid: ""
    status: NotReady
    portmap:
    envskey:
      SUPB_GUID: ""
    cmd:
    - /bin/sh
    - boot.sh


ip在环境变量
生成tag: host + port 

电调
方向舵机
摄像头舵机-上
摄像头舵机-下




1] sync info: %v&{map[01ad28f000:2 01d3dba500:2] map[01ad28f000:{0-95 1} 01d3dba500:{0-95 1}]}  function=agent.heartbeat


空包打时间戳
- 探测延迟

发送图片时间戳差异大
接收图片时间戳差异不大
  - 调整buffer 



- 子系统代码量，语言
  - 代码量: -
  - 语言: go  
- 子系统功能列表

| 功能     |           | 说明                                             |
| :------- | :-------- | :----------------------------------------------- |
| 调控功能 | 容器级别  | 基于docker api的容器生命周期管理                 |
|          |           | 基于cgroup的容器CPU、内存、I/O资源管理           |
|          |           | 基于tc工具的上行网络带宽、延迟控制               |
| 监控功能 | Host级别  | Host主机的CPU、内存、网络、I/O资源监控           |
|          | 容器级别  | 基于docker api的容器CPU、内存、网络、I/O资源监控 |
|          |           | 基于docker api的容器运行log监控                  |
|          | 应用级别  | 基于opentracing/jaeger的链路延迟监控             |
| 系统功能 | 状态管理  | 容器运行状态的心跳监控                           |
|          | RPC       | 基于总线的远程调控方法调用                       |
|          | 接入/退出 | 动态地控制主机、容器等再系统中的接入/退出        |

边缘服务器

CICD
形式化验证

agent-2021
- 信息高铁测调节点代理研究，实现调控数据、日志、延迟时间等跨区域、跨平台、跨设备的执行与采集。
agent-2022
- 信息高铁测调节点代理迭代，控制上实现OpenStack虚拟机、K8s容器编排系统的接入, 监控上对资源、日志、追踪的数据格式进行统一化方便存储与检索, 同时优化节点代理, 增强鲁棒性、稳定性, 降低资源开销, 实现性能的提升
- 信息高铁测调节点代理迭代, 实现对虚机、集群的控制，着力端到端追踪功能完善，并对其他指标进行标准化，优化节点代理，增强稳定性，提高性能
- 技术路线
  - 节点代理已有功能能够对容器、节点进行的监控与控制，但为契合信息高铁的发展以及当前云计算技术发展环境，节点代理应当提供对于虚机及集群的控制能力，来增加调控的维度，参考cncf目录，目前可行的技术有openstack，kubernetes与kubevirt，其分别对应于虚拟机平台，集群容器编排与尚未容器化的虚拟机编排，而在监控上，当前基于jaeger的链路追踪系统仍有不足之处，而作为接替Opentelemetry则能够提供更长久的技术支持与功能覆盖
    - OpenStack始于2010年，是Rackspace和美国国家航空航天局的合作项目，它主要作为基础设施即服务（IaaS）部署在公用云和私有云中，提供虚拟服务器和其他资源给用户使用。OpenStack平台由相互关联的组件组成，控制着整个数据中心内不同的厂商的处理器、存储和网络资源的硬件池。用户可以通过基于网络的仪表盘、命令行工具或RESTful网络服务来管理。
    - KubeVirt 是 Kubernetes 的虚拟机管理插件。目的是为 Kubernetes 之上的虚拟化解决方案提供一个共同基础。KubeVirt 的核心是通过 Kubernetes 的自定义资源定义 API 添加额外的虚拟化资源类型（尤其是 VM 类型）来扩展 Kubernetes。通过使用这种机制，Kubernetes API 可用于管理这些 VM 资源以及 Kubernetes 提供的所有其他资源。
    - OpenTelemetry 是一组 API、SDK、工具和集成，旨在创建和管理遥测数据，例如跟踪、指标和日志。该项目提供了一个与供应商无关的实现，可以将其配置为将遥测数据发送到用户选择的后端
  - 本项目着力节点代理的优化与功能强化，为顶层设计提供更丰富的监控维度与调控方式，满足当前及未来的需求


每台node上都安装agent
agent相互通信
Pod网络

opentelemtry 和 opentracing 和 jaeger client的关系

[openshift](https://www.tutorialspoint.com/openshift/openshift_docker_and_kubernetes.htm)



Jaeger 多用户
- agent 性能问题
  - api server 过多
  - 改进: 由agent统一接收请求，并对接入单位进行rpc调用   

- 控制中心需要提供 从 guid 查找根 agent 的功能
- 总线需要提供 guid 路由功能

- agent 先启动，后注册，不进行yaml文件的解析，只提供远程的调用


- helm 配置，k8s yamls
  - agent执行一个远程的操作
    - helm查询
    - 启停

- manager collector
  - 与 jaeger 

- 容器控制
- helm启停

- agent合并总控数据格式

- otrace，epuf
  

- agent 开发
  - 完成container所有功能
  - 完成所有测试

- helm 调研
  - helm环境搭建
  - helm操作调研
  - helm接入方式设计


- 工作
  - 开发工作
    - agent v1 收尾
    - agent v2 开发
  - 调研工作
    - ray调研
      - 用户
      - 系统(ing)
  - tracing调研
    - 概述
    - sampling(ing)
  - helm调研(ing)
- 计划
  - 开发
    - 短期
      - agent v0.2 版本测试
      - helm接入
      - 相关Demo开发
    - 长期
      - kubevirt接入
      - 子豪师兄
      - 云os功能支撑
  - 学习
    - 学校课程学习
  - 研究
    - tracing相关，adaptive sampling
    - unikernel 与 kubevirt




1. 每周进度
2. 分享
   - 王蕊老师: Splunk 调研汇报
   - 方浩镭: Agent v0.2 设计
3. 测调系统后续开发讨论

- agent 抽象
  - agent core抽象

- 其他
  - 工作汇报

- agent暂时不考虑 collector的数据聚合
  - reporter

- fast2022

- splunk lightstep 追踪系统


- 3月
  - v2版本重构基本完成(100%)
    - 完成 v2 版本重构文档
    - 完成基本容器监控、调控功能
  - agent性能测试 （20%）
    - 空载: 2-3%
    - 高负载: -

- 4月
  - k8s agent core 实现(10%)
    - k8s manager 兼容 helm 与 yaml 的启动方式
    - log 监控基于 k8s log
    - metric 监控基于 kube-prometheus
  - agent 数据转发监听端口
    - agent提供数据转发功能，通过暴漏接口的方式
  - agent bench mark
  - agent 开发框架


- 定制 jaeger collector 测试
- epbf exporter
- vector docker
- python profile
- prometheus server http handler

- agent 分布式方案
  - 每台Node上运行一个Agent
  - 局域网中，存在一个"网关角色", 运行在网关上的agent就是agent master，其余agent 作为slave 
  - 职责区分
    - agent master 负责启动 prometheus server 与 jaeger collector
      - 两者会将采集的数据发送到控制中心，并能够被控制中心控制
      - 两者都需要在启动是进行配置
        - Prometheus: 需要配置 job 以进行 exporter 的发现
        - Jaeger collector: 需要配置guid
    - agent slave 负责启动 prometheus exporter 与 jaeger agent 
      - Prometheus Exporter
        - 需要同步 ip:port 给 agent master
      - Jaeger Agent
        - 需要同步从Master 同步 Jaeger Collector URL
  - 分布式实现方案
    - Prometheus
      - 方案一，agent slave push
        - agent slave 启动完成 exporter 之后，提交 job 给 agent master
          - job_name: guid + expotername
          - instance: ip:port
        - agent master 收到job之后，修改 prometheus server config ，然后重启prometheus server
          - agent master 需要开启监听端口
      - 方案二，agent master pull
        - agent 通过配置获得 agent slave，拉取对应job信息
          - agent slave 需要开启监听端口
        - 信息拉取完毕后，agent server 再重启 prometheus server
      - 方案三，thrid database
        - agent slave 启动 exporter 之后，提交信息到 database
        - agent master监听数据改动，重启prometheus server

- agent v 0.3
  - api server -> controller
  - agentcore -> service
  - manager/collector -> component
  - dirver -> utils


- agent提供服务，用类别进行单独区分，作为微服务
  - 先使用协程替代
- agent运行过程中传递一种上下文，使得agent功能可以在运行过程中变化
- 每个节点上都允许daemon server
  - 注册host
  - 提供 run agent api
  - 从控制中心接收 agent yml， 启动对应的服务
    - 运行过程中，允许重载agent功能

- python程序监控
  - 监控程序调用栈及其延时，将监控数据反馈给控制中心
  - 关联延时与代码行，能够将关键代码行信息反馈给控制中心


- 总线需求
  - ISubscribe bug
    - （待解决）
  - RPC框架需求
    - 提供类似于http路由的多路复用功能
      - 下版本提供
    - 更多的参数传递方式
      - 路径参数、query参数等
      - 考虑提供


# 4.22-组会-记录 

- 主持人：丁兆云
  - 记录人：方浩镭

- 下次主持人：常子豪 
  - 下次记录人：白冰怡 

## 过进度
 
- 子豪师兄
  - 介绍合作项目，为大规模分布式验证平台，分布式场景的安全增强，与当前项目结合性未知
  - 正在准备系统领域(osdi、sosp)的调研分享
- 丁兆云老师
  - 资源展示图优化，需要使用Canvas进行实现，类似于子豪师兄PPT中的示意图
    - 一个柱子对应一个机器，机器之中是应用(容器)
  - DAG图优化，按照Splunk中的图例
- 方浩镭
  - linkerd在项目中的使用
- 马英杰
  - 日志展示页面优化，日志详细部分显示太窄

## 分享 

- 丁兆云——前端分享(Vue与React)
  - PPT优化，结构，逻辑，图例


kubectl -n test-helm-social-network port-forward --address=0.0.0.0 svc/nginx-thrift 8080
./wrk -D exp -t 1 -c 1 -d 20 -L -s ./scripts/social-network/compose-post.lua http://172.16.31.39:8080/wrk2-api/post/compose -R 1
./wrk -D exp -t 1 -c 1 -d 20 -L -s ./scripts/social-network/read-home-timeline.lua http://172.16.31.39:8080/wrk2-api/home-timeline/read -R 1
./wrk -D exp -t 1 -c 1 -d 20 -L -s ./scripts/social-network/read-user-timeline.lua http://172.16.31.39:8080/wrk2-api/user-timeline/read -R 1




python3 scripts/init_social_graph.py --graph=<socfb-Reed98, ego-twitter, or soc-twitter-follows-mun>


- socialnetwork镜像替换

- agent 扩展方式
  - 源码修改
  - 方法注册
    - agent 提供注册接口与sdk
    - 用户通过sdk注册方法到agent，并于agent保持通信

- CProfile场景
  - 实现机制
    - 用户使用 gluenet profile 模块
      - 抓取数据，并发送到总线(直接/间接)
  - 数据规约
    - Profile数据解析


- todo 容器启动默认资源值设置

sudo mount -o remount,rw '/sys/fs/cgroup'
sudo  ln -s /sys/fs/cgroup/cpu,cpuacct /sys/fs/cgroup/cpuacct,cpu

152.136.134.100:10048/jaeger-collector-glunet:latest
152.136.134.100:10048/glue_node_exporter:v0.1
docker.io/google/cadvisor:latest
quay.io/coreos/etcd:v3.4.14
jaegertracing/jaeger-agent:latest
prom/prometheus:latest
timberio/vector:0.21.0-distroless-libc

- 面向云原生场景的工具箱与一站式可观测性方案


- 餐品排行


```shell
# image-server
docker run -it --rm -p 10121-10125:10121-10125 -e SUPB_SERVICE_PORT=10121 -e SUPB_MIN_PORT=10122 -e SUPB_MAX_PORT=10125 152.136.134.100:10048/glue-image-server:latest

# car-back
docker run -it --rm -p 10120:8080 -e SERVICE_PORT=8080 --privileged 152.136.134.100:10048/car-back:latest

# car-front
docker run -it --rm -p 10126:8080 -e CAR_PATH=152.136.134.100:10120 152.136.134.100:10048/car-front:latest
```
JAEGER_AGENT_PORT: ""
JAEGER_AGENT_HOST: ""


- openshift
- 更长的 cprofile
- agent 热配置重载
- agent 集群方案细化
- agent proxy功能强化


- agent cache 使用 namespace进行区分(agent system 与 normal)

- linkerd启动脚本
  - linkerd install
  - linkerd viz install
- linkerd inject / uninject
- linkerd关闭脚本


- 孙院士您好，我是计算所南研院的一名学生，我想问一个问题,
- 现在软件行业发展这么快，其实很大原因就是各种各样的开源软件，特别是像Linux这样大型开源项目
- 这几年从开源指令集架构risc-v，再到咱们所包老师的开源芯片香山，我想问问就是，硬件开源，会不会跟Linux一样，这对于未来的计算机发展会有怎样的影响，一般来说, 然后,芯片架构往往是公司不公开的核心技术, 孙院士您又是如何看待开源芯片的呢。


- cprofile 使用域名解析前端

{
  "category": "荤菜",
  "describe": "使用鲍鱼，鱼翅等做的智商税",
  "dish_name": "佛跳墙",
  "image_id": "test",
  "price": 99
}

{
  "category": "荤菜",
  "describe": "正宗新疆大盘鸡",
  "dish_name": "大盘鸡",
  "image_id": "test",
  "price": 22
}

{
  "category": "荤菜",
  "describe": "使用老坛酸菜做的鱼",
  "dish_name": "酸菜鱼",
  "image_id": "test",
  "price": 6.5
}

{
  "area": "主窗口",
  "dishes": [
    "caq7g5pcllbd42msh4fg"
  ],
  "menu_name": "周一早餐",
  "period": "7:00 - 9:00 AM"
}

{
  "area": "主窗口",
  "dishes": [
    "caq7g5pcllbd42msh4fg"
  ],
  "menu_name": "周一午餐",
  "period": "11:00 - 13:00 AM"
}

{
  "area": "主窗口",
  "dishes": [
    "caq7g5pcllbd42msh4fg"
  ],
  "menu_name": "周一晚餐",
  "period": "17:00 - 19:00 PM"
}


# 6.24-组会-记录 

- 主持人：李柏桦
  - 记录人：方浩镭

## 过进度
 
- 桂老师
  - 南京服务器端口重新编排，空余端口用来搭建VNC
- 白冰怡
  - 下周一发初版的视频
- 丁兆云
  - 同时支持图形界面与Terminal
  - xbase4
- 翰霖
  - 使用linkerd监控未埋点的Serve
  - linkerd前端显示优先级提高

## 分享 

- 丁兆云
  - Vue相关内容

- 主持人：李柏桦
- 记录人：方浩镭
- 下次主持人：白冰怡
- 下次记录人：桂祥
- 组会人员：参与11人

## 一、工作进度讨论

1. 桂祥

- VNC相关内容
  - 重新编排南京服务器的公网端口，空出来的端口用来进行VNC的访问

2. 白冰怡

- 录屏相关
  - 讨论录屏脚本，下周一在工作群中发布视频，与大家讨论

3. 丁兆云

- WebConsole相关
  - 需要同时支持图形界面(通过VNC)与Terminal
  - 图形界面可以使用简单的 xfce4

4. 杜翰霖

- Serve对接相关
  - Serve可以使用linkerd进行未埋点时的trace监控
  - linkerd前端显示在需求池中的优先级提高
 
## 二、分享

- 分享人：丁兆云
- 主题《Vue原理介绍》

- 启动helm之后，使用 k8s driver，获得namespace下的所有pod
  - 返回 <guid, podname, containername> 列表

- [k8s bmc](https://github.com/phoenixnap/k8s-controller-bmc)

- Guid 与 Namespace，Name的关系
1. Guid 由 Namespace，Name组成
2. 拆分成数据结构

- Guid的命名粒度，Guid之间的关联关系
  - Namespace具有强相关性

```go
map[guid]map[resource] interface{}
```

- 索引
  - Agent Guid
  - 应用域(Namespace)
  - 名称

- 注册GUID 
  - 返回配置

- 启动参数
  - 选择本地配置
    - 覆盖远端配置
  - 选择远端配置


- agent 拉取远端配置
- manager 根据 <role, platform> 生成 agent, boot 配置

- k8s 平台监控组件启动脚本
- docker 平台监控组件启动脚本

docker run --restart=always --network host -d -v /home/docker/frp/frpc.ini:/etc/frp/frpc.ini --name frpc snowdreamtech/frpc

INSERT INTO `glue`.`area`(`area_id`, `country`, `province`, `city`, `county`) VALUES ('030001', 'china', 'shanxi', 'xin', 'xinghualing')

- heartbeat exporter
- helm release启动时，获取container所在的node


- [telegraf](https://docs.influxdata.com/telegraf/v1.15/plugins/#input-plugins)

- [x] kubeconfig动态配置
- [x] 启动脚本路径动态配置
- [x] 导入应用时查询container所属pod所在的nodename
- [] 启动应用时查询container所属pod所在的nodename
- [x] 北京小车部署agent
- [ ] 完善部署脚本docker: 可重复使用，响应变化, 从环境变量获得hostname
- [ ] 集群来源标签 origin[platform, external]
- [x] k8s Vector Guid植入
- [ ] kubernetes console
- [x] arm脚本运行失败问题
- [ ] watch dog
- [ ] nodeexporter开启perf监控


- Cache只缓存meta与status信息，关键字字段
  - 同时，也只持久化meta信息
- 其他信息通过相应组件查询获得

- k8s 配置文件导出
  - [https://www.kubernetes.org.cn/doc-58]



```
alias kubectl='kubectl --kubeconfig /kube/config'
alias linkerd='linkerd --kubeconfig /kube/config'

wget -qO - https://gist.githubusercontent.com/Faione/730269a0af41a57456d4a1469ed09b81/raw/8cb25dc94898f8b8404903418f212a4a1925e530/init.sh | sh
```

alias kubectl='kubectl --kubeconfig /kube/config'

- agent重构
  - RPC
    - 框架更新
    - 依赖注入机制
  - 架构
    - 领域驱动模型，service逻辑拆分
  - 存储
    - 保存meta, status, 关键索引
      - 持久化到manager中
      - 本地存储
    - 底层数据与驱动同步
- 配置仓库
  - 配置仓库(git/redis)
    - rules
    - configs
  - rules cicd
- 监控组件
  - exporter列表
    - exporter配置
  - 自定义exporter开发方式
- 监控启动方式与配置加载
  - Operator(调研)
  - Agent(调研+实现)

- 官方node_exporter无法启动
- 源码编译
  - vscode中能够启动node_exporter
  - mobaxterm 中无法启动 node_expoter


- [x]整理组会记录


- 主持人：钱子航
- 记录人：方浩镭
- 下次主持人：高梓源
- 下次记录人：林纪涵
- 下次分享者: 唐诗博

## 过进度

1. 唐
  - 调研项目需要提前进行沟通，避免重复
2. 王
  - 低代码开发推至10月份
3. 林
  - 重点在于测试物端，以及kubeedge中，k8s不涉及的部分
4. replay相关
  - replay每两周进行一次
  - 按前端、后端的顺序设置主题

## 分享

1. 镜凯分享《Serve》
  - 目前linkerd可以对已存在的pod注入相关配置，不满足Serve动态运行pod的需求
  - 需要针对Serve进行优化，如对特定的namespace注入linkerd相关配置，以启动linkerd auto inject
## Replay

1. 任务分配应按照任务的类型分配给适合的人
2. 当前项目需要建立于一个规范，如commit命名，commit的内容，分支管理等
   - [k8s代码贡献规范](https://kubernetes.io/docs/contribute/) 
3. gluenet核心在于通过api调用融合各个系统
   - 如通过调用openshift的接口，在gluenet中实现cicd功能


系统测试阶段
- []测调组准备linkerd测试样例，部署并注入linkerd配置
- []Paas组进行linkerd viz部署
- []测调组连通prometheus与linkerd viz组件，测试数据聚合

- service mesh调研
- linkerd测试样例
- perf方案
  - perf容器，能够默认执行perf stat，同时将数据发送出去

查询逻辑
- 总控查询platform为"Maas"的agent，获取所有符合条件的agent所管理的机器

- overlay网络
  - 服务分组
  - 组间访问策略
- cluster mesh
  - 物理机代理




```shell
$ docker run -itd \
 --privileged \
 -e WATCHER_PUSH_GATEWAY="http://172.16.31.38:27182" \
 -e WATCHER_CUSTOM_LABELS="tasknum=${i};hostname=1038" \
 -e WATCHER_LOG_DEBUG=true \
 -e GUID="test-mpi-guid" \
 -e JOB_NAME="perf-mpi-demo" \
 mpi:100902 \
 inspect.sh "mpirun -np 2 /NPB3.3.1/NPB3.3-MPI/bin/is.C.2"

$ docker run -itd \
 --privileged \
 -e WATCHER_PUSH_GATEWAY="http://172.16.31.38:27182" \
 -e WATCHER_CUSTOM_LABELS="tasknum=1;hostname=1038" \
 -e WATCHER_LOG_DEBUG=true \
 -e GUID="test-mpi-guid" \
 -e JOB_NAME="test-mpi-guid/1" \
 mpi:100902 \
 inspect.sh "mpirun -np 2 /NPB3.3.1/NPB3.3-MPI/bin/is.C.2"
```

[rdt技术架构](https://cloud-atlas.readthedocs.io/zh_CN/latest/kernel/cpu/intel/intel_rdt/intel_rdt_arch.html)

manager helm 部署
- 挂载目录权限应当为 `775`

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: frpc-tencent-config
data:
  frpc.ini: |
    [common]
    tls_enable = true
    server_addr = 129.226.101.5
    server_port = 17000
    token = gluenet123@aliyun
    
    [glue-core]
    local_ip = glue-core
    local_port = 10000
    remote_port = 30000

    [glue-apiserver]
    local_ip = glue-apiserver
    local_port = 10000
    remote_port = 30001

    [casdoor-svc]
    local_ip = casdoor-svc
    local_port = 10000
    remote_port = 30002

    [glue-ui]
    local_ip = glue-ui
    local_port = 80
    remote_port = 30003

    [glue-guid-service]
    local_ip = glue-guid-service
    local_port = 18888
    remote_port = 30004

    [glue-data-mysql-service]
    local_ip = glue-data-mysql-service
    local_port = 3306
    remote_port = 30005
    
    [glue-web-console]
    local_ip = glue-web-console
    local_port = 10000
    remote_port = 30006
    
    [glue-stream-service]
    local_ip = glue-stream-service
    local_port = 10000
    remote_port = 30008

    [range:glue-stream-prom-service]
    local_ip = glue-stream-prom-service
    local_port = 10000-10002
    remote_port = 30009-30011

    [range:casnode-svc]
    local_ip = casnode-svc
    local_port = 10000-10001
    remote_port = 30012-30013
    
    [glue-etcd]
    local_ip = glue-etcd
    local_port = 2379
    remote_port = 30014
    
    [grafana-svc]
    local_ip = grafana-svc
    local_port = 3000
    remote_port = 30016
    
    [glue-apis]
    local_ip = glue-apis
    local_port = 10000
    remote_port = 30007

    [range:glue-data-influxdb-service]
    local_ip = glue-data-influxdb-service
    local_port = 8086,10000
    remote_port = 30050,30051
    
    [range:glue-data-influxdb-log-service]
    local_ip = glue-data-influxdb-log-service
    local_port = 8086,10000
    remote_port = 30060,30061
    
    [sponge-service]
    local_ip = sponge-service
    local_port = 8087
    remote_port = 30021
    
    [glue-data-es-service]
    local_ip = glue-data-es-service
    local_port = 9200
    remote_port = 30023

    [glue-data-kibana-service]
    local_ip = glue-data-kibana-service
    local_port = 5601
    remote_port = 30024
    
    [range:glue-data-jaeger-ui-service]
    local_ip = glue-data-jaeger-ui-service
    local_port = 16686-16687
    remote_port = 30025-30026
    
    [glue-data-registry-service]
    local_ip = glue-data-registry-service
    local_port = 8080
    remote_port = 30028
    
    [docker-registry]
    local_ip = docker-registry
    local_port = 5000
    remote_port = 10048
    
    [glue-srvm]
    local_ip = glue-srvm
    local_port = 10000
    remote_port = 30038
    
    [glue-srvp]
    local_ip = glue-srvp
    local_port = 10000
    remote_port = 30022
```

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: frpc-tencent-config
data:
  frpc.ini: |
    [common]
    tls_enable = true
    server_addr = 129.226.101.5
    server_port = 7000
    token = gluenet123@aliyun

    [range:glue-stream-prom-service]
    local_ip = 10.105.214.213
    local_port = 10000-10002
    remote_port = 30009-30011
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frp-client
spec:
  selector:
    matchLabels:
      app: frp-client-app
  replicas: 1
  template:
    metadata:
      name: frp-client
      labels:
        app: frp-client-app
    spec:
      containers:
        - name: frp-client
          image: snowdreamtech/frpc:latest
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - mountPath: /etc/frp/
              name: config
      volumes:
      - name: config
        configMap:
          name: frpc-tencent-config
          items:
          - key: frpc.ini
            path: frpc.ini
```

agent调试部分：
- [x] 1，prometheus走mquery指标查询总控
- [x] 2，prometheus走mquery指标查询agent
- [x] 3，jaeger 走 mquery查询agent
- [x] 4，cilium 走 mquery查询agent
- [x] 5，cprofile 走 mquery 查询agent
- [x] 测试 level 3 查询


cp ~/.kube/config deploy/helm/manager/v0.3/k8sconfig


网络代理导致无法通过 ip 访问 容器服务

- [ ] helm in cluster configuration
- [ ] pod ci

- 高老师项目(prometheus)，天气预报项目(prometheus + jaeger)
- 北京集群监控
- envoy部署


vector

```shell
NAMESPACE=prometheus
RELEASE=vector
CHART=deploy/monitor/helm/vector/
AGENT_GUID=fdfec62eb21cf0ef0a22c690f40774de

NATS_URL="nats://172.16.31.39:14222"
NATS_TOPIC="guid.manager:rpc.apis.data.logs.push"

helm -n ${NAMESPACE} install \
--create-namespace \
--set agentGuid=${AGENT_GUID} \
--set customConfig.sinks.nats.url=${NATS_URL} \
--set customConfig.sinks.nats.subject=${NATS_TOPIC} \
${RELEASE} ${CHART}
```

prometheus

```shell
NAMESPACE=prometheus
RELEASE=kube-prom-stack
CHART=deploy/monitor/helm/prometheus/
AGENT_GUID=fdfec62eb21cf0ef0a22c690f40774de

NATS_URL="nats://172.16.31.39:14222"
NATS_TOPIC="guid.manager:rpc.apis.data.logs.push"

helm -n ${NAMESPACE} install \
--create-namespace \
--set grafana.enabled=false \
--set alertmanager.enabled=true \
--set defaultRules.rules.gluenet.agent=${AGENT_GUID} \
--set glue-transfer.nats.url=${NATS_URL} \
--set glue-transfer.nats.topic=${NATS_TOPIC} \
${RELEASE} ${CHART}
```

## sv39

> 当RWX全为0时，则代表该PTE存储的地址是下一级页表的物理地址，否则代表当前页表是最后一级页表

virt: 0x8000_0000 -> 0x1000_0000_0000_0000_0000_0000_0000_0000

PT:  0_0000_0000
PMD: 0_0000_0000
PGD: 0_0000_0010 (0x2)

virt: 0xffff_ffc0_8000_0000 -> 0x*1111_1100_0000_1000_0000_0000_0000_0000_0000_0000_0000

PT:  0_0000_0000
PMD: 0_0000_0000
PGD: 100_0000_10 (0x102)

virt: 0xffff_ffff_c000_0000 -> 0x*1111_1111_1100_0000_0000_0000_0000_0000_0000_0000

PT:  0_0000_0000
PMD: 0_0000_0000
PGD: 111_1111_11 (0x1ff)

## sv48

virt: 0x8000_0000 -> 0x*0000_1000_0000_0000_0000_0000_0000_0000_0000

PT:  0_0000_0000
PMD: 0_0000_0000
PGD: 0_0000_0010 (0x2)
PUD: 0_0000_0000 (0x0)


virt: 0xffff_ffc0_8000_0000 -> 0x*1111_1100_0000_1000_0000_0000_0000_0000_0000_0000_0000

PT:  0_0000_0000
PMD: 0_0000_0000
PGD: 100_0000_10 (0x102)
PUD: 1_1111_1111 (0x1ff)

virt: 0xffff_ffff_c000_0000 -> 0x*1111_1111_1100_0000_0000_0000_0000_0000_0000_0000

PT: 0_0000_0000
PMD: 0_0000_0000
PGD: 111_1111_11 (0x1ff)
PUD: 1_1111_1111 (0x1ff)

000002
1ff102
1ff1ff


## sv39

virt: 4000_0000

PGD: 1

 1 0*18


账号：tktmxevcsx@cnevewiki.com
密码：&OP5FE!&pPvi


qemu-system-x86(19517)─┬─{CPU 0/KVM}(19530)
                       ├─{IO mon_iothread}(19528)
                       ├─{call_rcu}(19523)
                       └─{vnc_worker}(19532)

华为云项目所内资源:
1. 数据分析平台 Jupyter Lab:
  - url: http://10.208.130.243:8888/
  - token: be020bad993223fe391b93839fd9957d469c740983f43508
  - 目录说明
    - appProfile: 当前用于进行应用画像，分析的代码
    - document: 相关文档
      - DoubleWeekReports: 双周汇报ppt
      - Metrics: 当前支持的指标及其说明
      - Papers: 相关论文
      - PreWorks: 所内相关工作
2. 数据展示平台 Grafana:
  - url: http://10.208.130.243:3001/
  - uname/passwd: admin/admin
  - DashBoards
    - ALL IN ONE!: 展示所有指标
    - Application: 展示app QoS指标
    - Host: 展示宿主机指标
    - Node: 同上
    - Offline Analysis: 0721竖亥benchmark数据展示
    - VM: 展示虚拟机指标
3. 数据查询平台 Prometheus:
   - url: http://10.208.130.243:9090/
   - 使用参考: https://prometheus.io/docs/prometheus/latest/querying/examples
   - 指标参考 Jupyter Lab 中的 Metrics 或 Grafana 中的正在使用的 Metric
4. 代码仓库
   - 内部仓库: http://10.208.130.243:3000/explore/repos
   - mirror仓库: 
     - AppProfile: https://github.com/Faione/WorkloadAnalysis
     - Grafana Dashboards: https://github.com/Faione/grafana_dashboards
5. 镜像仓库
   - Harbor: https://10.208.130.243/
   - uname/passwd: admin/Harbor12345


```
tymm135 
pbxj928
Cqmyg@817
```

Future-Proofing Virtual Memory

Ai growth 8x per y
Memory in DC is on the critical path

Target: App don not care aboout where memory is 
Cur: 
  - increasing
  - V M Scaling
    - capacity significantly increased
    - scaleble
  - TLB increasing

No TLB Memory

- Performance
  - Core directly interact withc cache hierarchy
  - Page-based trans/protect
- HW
  - TLBs do not scale, cannot provide the req coverage
    - ?Memory is Big enought and do not need Virtual Memory, thus no need for TLBs, and using other tech to protect
    - Keep virtual memory and change the implementation
    - ?Big Page can decrease TLBs Promblem(Cache More Memory)
- Problem
  - TLBs is the bottleneck
  - Cache Hierarchy Cap Increase -> time spent in data accesses goes down（slow?） -> virtual memory overhead increase

- Midgrad
  - using a global vm layout to store all vmas(vm area)
  - ?VLB: translate vma to midgard vm

- Conclusion
  - VM is an important abstraction
  - VM implementations have faltered with memeory and chace scaling
  - Midgrad accelerates VM
    - using fast VMA-based translations
    - no application modifications



7.5-5(经验少)-8(经验丰富)-7(验证相关经验不多)-6.5(基础不太扎实)-(gap)-6(方向不match)-7.5(验证经验不多)-..

5-5


base 3200
128 d: min 3184
128 s: min 3191
256 s: min 3147
256 d: min 3147
512 s: min 2967
512 d: min 2949

[avx_blog](https://travisdowns.github.io/blog/2020/01/17/avxfreq1.html)
[.net_7_avx](https://learn.microsoft.com/zh-cn/dotnet/api/system.runtime.intrinsics.x86.avx2?view=net-7.0)

Gottschlag M, Machauer P, Khalil Y, et al. Fair Scheduling for {AVX2} and {AVX-512} Workloads[C]//2021 USENIX Annual Technical Conference (USENIX ATC 21). 2021: 745-758.

AVX2 and AVX-512 Scheduling
Fair Scheduling for AVX2 and AVX-512 Workloads
年份: 2021
期刊: ATC
主题: 公平调度
核心方法: AVX2/AVX512检测、vruntime修正
简述:
部分AVX指令使用时会导致CPU turbo频率下降，且频率的恢复有一定延迟，这会影响到HyperThread及相同CPU上后续任务的性能，本论文通过监测向量寄存器的使用，发现使用AVX指令的任务，并根据离线的性能分析对频率、AVX指令时间片进行建模，从而能够在线地计算当前CPU所处turbo级别，根据频率比例对vruntime进行修正，保证调度的公平性。
Abstract:
CPU schedulers such as the Linux Completely Fair Scheduler try to allocate equal shares of the CPU performance to
tasks of equal priority by allocating equal CPU time as a
technique to improve quality of service for individual tasks.
Recently, CPUs have, however, become power-limited to the
point where different subsets of the instruction set allow for
different operating frequencies depending on the complexity
of the instructions. In particular, Intel CPUs with support for
AVX2 and AVX-512 instructions often reduce their frequency
when these 256-bit and 512-bit SIMD instructions are used
in order to prevent excessive power consumption. This frequency reduction often impacts other less power-intensive
processes, in which case equal allocation of CPU time results
in unequal performance and a substantial lack of performance
isolation.
We describe a modification to existing schedulers to restore
fairness for workloads involving tasks which execute complex power-intensive instructions. In particular, we present a
technique to identify AVX2/AVX-512 tasks responsible for
frequency reduction, and we modify CPU time accounting
to increase the priority of other tasks slowed down by these
AVX2/AVX-512 tasks. Whereas previously non-AVX applications running in parallel to AVX-512 applications were
slowed down by 24.9% on average, our prototype reduces the
performance difference between non-AVX tasks and AVX-
512 tasks in such scenarios to 5.4% on average, with a similar
improvement for workloads involving AVX2 applications.


Yang X, Blackburn S M, McKinley K S. Elfen Scheduling:{Fine-Grain} Principled Borrowing from {Latency-Critical} Workloads Using Simultaneous Multithreading[C]//2016 USENIX Annual Technical Conference (USENIX ATC 16). 2016: 309-322.

Elfen
Elfen Scheduling: Fine-Grain Principled Borrowing from Latency-Critical Workloads Using Simultaneous Multithreading
年份: 2016
期刊: ATC
主题: 应用共置、干扰隔离
核心方法: principled borrowing、nanonap
简述:
Hyper-Threading能够提升CPU性能，但仍共享了部分资源因此存在竞争，这可能引起LC应用延迟的上升。Elfen为LC与Batch应用在Hyperthread上共置的性能问题，其先实现了nanonap系统调用，能够快速的释放CPU资源，但同时仍然保存对CPU的占用，然后再向Batch应用中注入检测代码，当发现Hyperthread另一侧的LC应用运行时，就立刻出让资源，反之则借用资源并执行。
Abstract:
Web services from search to games to stock trading impose strict Service Level Objectives (SLOs) on tail latency.
Meeting these objectives is challenging because the computational demand of each request is highly variable and
load is bursty. Consequently, many servers run at low
utilization (10 to 45%); turn off simultaneous multithreading (SMT); and execute only a single service — wasting
hardware, energy, and money. Although co-running batch
jobs with latency critical requests to utilize multiple SMT
hardware contexts (lanes) is appealing, unmitigated sharing of core resources induces non-linear effects on tail
latency and SLO violations.
We introduce principled borrowing to control SMT
hardware execution in which batch threads borrow core
resources. A batch thread executes in a reserved batch
SMT lane when no latency-critical thread is executing
in the partner request lane. We instrument batch threads
to quickly detect execution in the request lane, step out
of the way, and promptly return the borrowed resources.
We introduce the nanonap system call to stop the batch
thread’s execution without yielding its lane to the OS
scheduler, ensuring that requests have exclusive use of
the core’s resources. We evaluate our approach for colocating batch workloads with latency-critical requests
using the Apache Lucene search engine. A conservative
policy that executes batch threads only when request lane
is idle improves utilization between 90% and 25% on
one core depending on load, without compromising request SLOs. Our approach is straightforward, robust, and
unobtrusive, opening the way to substantially improved
resource utilization in datacenters running latency-critical
workloads.


turbo boost
- turbo boost 频率 与 perfmance 模式频率关系
- active core 与频率的关系
- cgroup绑定核心时出现抖动(跨CPU绑核)
- C7实例相比于C6s实例出现更严重的抖动


多干扰单个分析
- x 基于单个干扰敏感，使用 阈值 提取有代表性的指标，找出最优代表性的指标
- x 对指标进行提取，下结论
- - 单应用进行排序，多应用综合分析
- x 重绘雷达图 


华为合作一期验收会
- 议题一: 近5年数据中心节点侧QoS保障问题研究综述
- 议题二: 面向华为云典型负载的应用画像分析

交付代码布局
- main: 仅对分支说明
- monitor_stack: 监控架构部署脚本
- experiment: 实验分发脚本
- analysis: 数据采集、数据分析脚本
- *resctrl_exporter:
- *kvm_exporter:
- *kernel_exporter:
- *libvirt_exporter:
- *node_exporter: 
- *cgroup_modify

main monitor_stack experiment analysis resctrl_exporter kvm_exporter kernel_exporter libvirt_exporter node_exporter


微秒级调度文章


1. [Basu N, Montanari C, Eriksson J. Frequent background polling on a shared thread, using light-weight compiler interrupts[C]//Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation. 2021: 1249-1263.](https://dl.acm.org/doi/abs/10.1145/3453483.3454107)
2. [Demoulin H M, Fried J, Pedisich I, et al. When idling is ideal: Optimizing tail-latency for heavy-tailed datacenter workloads with perséphone[C]//Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles. 2021: 621-637.](https://dl.acm.org/doi/abs/10.1145/3477132.3483571)
3. [Humphries J T, Natu N, Chaugule A, et al. ghost: Fast & flexible user-space delegation of linux scheduling[C]//Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles. 2021: 588-604.](https://dl.acm.org/doi/abs/10.1145/3477132.3483542)
4. [Kaffes K, Chong T, Humphries J T, et al. Shinjuku: Preemptive Scheduling for {μsecond-scale} Tail Latency[C]//16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19). 2019: 345-360.](https://www.usenix.org/conference/nsdi19/presentation/kaffes)
5. [Yang X, Blackburn S M, McKinley K S. Elfen Scheduling:{Fine-Grain} Principled Borrowing from {Latency-Critical} Workloads Using Simultaneous Multithreading[C]//2016 USENIX Annual Technical Conference (USENIX ATC 16). 2016: 309-322.](https://www.usenix.org/conference/atc16/technical-sessions/presentation/yang)
6. [Iyer R, Unal M, Kogias M, et al. Achieving Microsecond-Scale Tail Latency Efficiently with Approximate Optimal Scheduling[C]//Proceedings of the 29th Symposium on Operating Systems Principles. 2023: 466-481.](https://dl.acm.org/doi/abs/10.1145/3600006.3613136)


华为合作一期验收会
  - 议题一: 近5年数据中心节点侧QoS保障问题研究综述
    - 汇报人: 杜翰霖; 时间: 30min
  - 议题二: 面向华为云典型负载的应用画像分析
    - 汇报人: 杜翰霖、方浩镭; 时间: 60min


- 在线指标采集系统
  - 背景
    - 初期没有实验数据
    - 线上数据分析难度大
    - 为二期项目进行准备
  - 目标
    - 统一格式、扩展性
    - 丰富度
      - 以虚拟机为目标，监测各个维度的指标信息
        - Host
        - Hypervisor
        - Kernel
        - App
    - 数据关联
      - vm cgroup -> pid -> kvm stat
  - 指标总览
- 实验设计
  - 负载选择
    - 竖亥
    - 计算所
  - 干扰选择
    - 干扰类型
    - 纯净干扰
- 数据集
  - 应用、干扰
  - 指标维度

- 线上业务
  - CPU核心数量越来越多
  - HPC上云越来越多(AVX指令使用越来越多)

```
qemu-system-x86_64 \
  -enable-kvm \
  -machine 'pc' \
  -cpu 'Nehalem' \
  -smp 8,cores=4 \
  -m 4G \
  -device virtio-blk-pci,drive=hd -drive file=image.qcow2,if=none,id=hd \
  -device virtio-net-pci,netdev=net \
  -netdev user,id=net,hostfwd=tcp::2222-:22 \
  -kernel kernel \
  -initrd initrd \
  -nographic \
  -append "root=LABEL=rootfs console=ttyS0"
```

```
qemu-system-x86_64 \
  -enable-kvm \
  -machine 'pc' \
  -cpu 'Nehalem' \
  -smp 8,cores=4 \
  -m 4G \
  -device virtio-blk-pci,drive=hd -drive file=alpine-uefi-2024-02-02.qcow2,if=none,id=hd \
  -device virtio-net-pci,netdev=net,ip=192.168.1.10/24 \
  -netdev bridge,br=br0,id=net \
  -kernel /home/fhl/alpine_temp/vmlinuz-virt \
  -initrd /home/fhl/alpine_temp/initramfs-virt \
  -nographic \
  -append "initrd=initramfs-virt  b71b7679-c278-4705-8187-097b41e329e7 rootfstype=ext4 modules=kms,scsi,virtio console=ttyS0"
```

- 共置问题
- 当前的解决方案
  - 应用配比
  - 调度策略
- 当前手段的不足
  - 不同调度算法对不同的共置策略效果不同
  - 使用全局调度队列在CPU数量较多时效果不佳，且难以区分不同的group
- Control Zone
  - 允许在host上存在不同调度策略
  - 对于不同的共置场景，提供不同的调度手段
  - 响应应用行为的变化，动态调整调度策略
- 实验验证

创新点
- Control Zone 设计与实现
- 针对 Control Zone (VM) 的监控系统，用以进行应用画像与分析，判断调度算法的性能
- 针对典型场景的调度算法设计

灵活性
- 不同流量的LC应用混部

提高对有限资源的利用率 -> 混部问题
- 选择哪些应用进行混部
  - 启发式: LC 与 BE
  - 资源使用倾向/干扰敏感度
- 调度策略(SLO保障)
  - Linux调度(CFS,EEVDF)
  - 资源隔离
  - 任务调度
    - 微秒级调度
  - 应用对不同调度策略的性能表现
- 存在的问题
  <!-- - 共置策略
    - 启发式的共置策略忽略了其他资源上共置的可能性 -> 多资源 -->
  - 调度策略(与微秒级调度对比的表格，原理)
    - 内核提供的调度策略在多数场景中有效，但灵活性差，难以修改
    - 内核外的调度策略灵活性好，但是调度延时高，因此效果不好
    - 微秒级调度能解决了如上问题，但兼容性差
  - 数据中心中应用种类多，共置场景复杂多变，单一调度策略无法在所有场景中都有效
- 微秒级调度绕过了内核，因此在性能与灵活性上更好，但也由于失去了一部分内核功能，导致兼容性变差

case设计
- 混部场景下，不能使用一种调度算法，不同场景有不同调度算法
  - 底层硬件：elfen
  - 应用层需求不同
  -> 不同场景有不同调度算法
现有调度算法无法满足动态要求
现有工作问题

-> 本文工作

- Control Zone(调度器级别 ->)
  - 通过eBPF技术来定制调度策略，同时实现用户态与内核态的协作
  - 使用虚拟机隔离不同的调度场景，然后对单一场景进行定制优化
  - 适应场景变化变更调度策略(区别于任务迁移)
    - LC应用不同负载强度时，采用不同的调度算法
  - 较强的隔离性也为对整体的资源使用监控提供了便利
  - 虚拟化/sched_ext 开销
- 在线监控系统
  - 应用画像
  - 应用劣化监测(envoy)
- 多资源感知调度策略
  - 进行中

Control Zone: 隔离空域
- 在隔离空域中，调度飞机起降，实现高效控制

Control Zone 分层调度
- 合理划分的Control Zone中，以任务切换调度为主
- 多个 Control Zone 并行运行时，以资源划分为主


Tips
- Control Zone中的CPU规模
  - 较小(0-8), 
 


czyaml

VM
- resource
- kernel
Pod
- service A
- service B
- port A
- port B

 czctl



多种调度算法
- 调度算法需要适应硬件的变化
- 应用的需求要求特别的调度算法
  - 应用对于调度算法的需求，重点在于算法而非配置

多样的混部场景

-> 服务器上的调度算法需要一个用户态、可配置、灵活切换的调度框架

PPT：
- 背景: 混部场景带来QoS保障问题
- 硬件差异对于调度的需求
- 软件差异对于调度的需求
- 单一调度算法不能满足所有场景，需要灵活的调度方式
  - ghOst
  - Sched_ext
- 小结
  - ghOSt兼容性较差，通知强制要求调度在用户态决策，因此性能较低
  - sched_ext兼容性较好，对于简易的调度算法，可以完全只在内核处理，但是设计上的考虑，无法同时运行不同的调度策略
- Control Zone实现
  - 虚拟机基于sched_ext实现，支持灵活调度
  - Host上可以同时运行多个虚拟机，从而支持同时运行多个算法
  - 支持的调度算法
    - Linux原生调度算法
    - 扩展的调度算法: 集中式、用户态调度
- 典型应用画像分析
  - 应用在敏感性/亲和性互补
- 后续计划
  - 适配更多的混部场景
  - 智能的混部场景识别


- 较小CPU规模的CZ: frontend monitor, backend monitor
  - vHost net
- 较大CPU规模的CZ: frontent monitor
  - SRIOV

- Fair 调度类作为保底调度
- Sched Ext用来通过调度手段，防止低优先级应用损害高优先级应用
- 结合eBPF在各个子系统中的观测，将优先级机制引入到各个子系统中
    - 在内存子系统中，让高优先级应用的内存分配被保障，或让高优先级应用的访存需求被保障
    - 高优先级应用运行时，使能 Sched Ext中对资源的限制 

- 互斥调度与 Idle 的区别
  - IDLE过于简单，不够灵活
  - 基于BPF Scheduler的互斥调度能够应对更多的互斥场景


TODO

CONFIG_STRIP_ASM_SYMS



--- 2

============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec
----------------------------------------------------------------------------------------------------------------------------
Sets         7627.73          ---          ---         1.19842         1.18300         1.59100         6.07900       587.56
Gets        76261.19         7.67     76253.52         1.19102         1.17500         1.58300         5.59900      2970.75
Waits           0.00          ---          ---             ---             ---             ---             ---          ---
Totals      83888.91         7.67     76253.52         1.19169         1.17500         1.58300         5.66300      3558.31

tx_kick_counts: 1739271
tx_net_counts: 0
rx_kick_counts: 39
rx_net_counts: 1643963
tx_ratio: 0.0000
rx_ratio: 42152.8974

--- 4

============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec
----------------------------------------------------------------------------------------------------------------------------
Sets        10093.34          ---          ---         1.80748         1.79100         2.49500         6.91100       777.42
Gets       100898.05        13.33    100884.72         1.80040         1.78300         2.47900         6.84700      3930.45
Waits           0.00          ---          ---             ---             ---             ---             ---          ---
Totals     110991.39        13.33    100884.72         1.80104         1.78300         2.47900         6.84700      4707.87

tx_kick_counts: 2086608
tx_net_counts: 0
rx_kick_counts: 48
rx_net_counts: 1648130
tx_ratio: 0.0000
rx_ratio: 34336.0417


2086608,0,48,1648130,110991.39,2.47900

--- 8

============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec
----------------------------------------------------------------------------------------------------------------------------
Sets        11958.58          ---          ---         3.04726         3.02300         3.87100         9.85500       921.02
Gets       119518.72        13.33    119505.39         3.04023         3.02300         3.83900         9.79100      4656.20
Waits           0.00          ---          ---             ---             ---             ---             ---          ---
Totals     131477.30        13.33    119505.39         3.04087         3.02300         3.83900         9.79100      5577.22

tx_kick_counts: 2237081
tx_net_counts: 0
rx_kick_counts: 46632
rx_net_counts: 345992
tx_ratio: 0.0000
rx_ratio: 7.4196

2237081,0,46632,345992,131477.30,3.83900

--- 16

============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec
----------------------------------------------------------------------------------------------------------------------------
Sets        12404.85          ---          ---         5.87860         5.85500         9.72700        19.58300       955.02
Gets       123914.11         0.00    123914.11         5.86239         5.85500         9.08700        19.83900      4827.53
Waits           0.00          ---          ---             ---             ---             ---             ---          ---
Totals     136318.95         0.00    123914.11         5.86386         5.85500         9.15100        19.83900      5782.54

tx_kick_counts: 2283153
tx_net_counts: 0
rx_kick_counts: 48938
rx_net_counts: 21548
tx_ratio: 0.0000
rx_ratio: 0.4403

2283153,0,48938,21548,136318.95,9.15100

--- 32

============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec
----------------------------------------------------------------------------------------------------------------------------
Sets        23970.72          ---          ---         6.08592         5.43900        12.35100        22.78300      1845.39
Gets       239442.56         0.00    239442.56         6.06691         5.43900        12.09500        22.78300      9328.32
Waits           0.00          ---          ---             ---             ---             ---             ---          ---
Totals     263413.28         0.00    239442.56         6.06864         5.43900        12.09500        22.78300     11173.71

tx_kick_counts: 2044619
tx_net_counts: 0
rx_kick_counts: 43561
rx_net_counts: 186942
tx_ratio: 0.0000
rx_ratio: 4.2915

2044619,0,43561,186942,263413.28,12.09500

--- 64

============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec
----------------------------------------------------------------------------------------------------------------------------
Sets        45070.62          ---          ---         6.50381         6.01500        19.19900        36.60700      3469.65
Gets       450171.38         0.00    450171.38         6.44867         6.01500        17.02300        33.79100     17538.10
Waits           0.00          ---          ---             ---             ---             ---             ---          ---
Totals     495242.00         0.00    450171.38         6.45369         6.01500        17.27900        34.04700     21007.75

tx_kick_counts: 953933
tx_net_counts: 0
rx_kick_counts: 4846
rx_net_counts: 636167
tx_ratio: 0.0000
rx_ratio: 131.2767

953933,0,4846,636167,495242.00,17.27900

--- 128 

============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec
----------------------------------------------------------------------------------------------------------------------------
Sets        55551.20          ---          ---        10.52770        10.04700        27.13500        88.06300      4275.24
Gets       554438.73         0.00    554438.73        10.47277         9.98300        25.59900        86.52700     21600.40
Waits           0.00          ---          ---             ---             ---             ---             ---          ---
Totals     609989.94         0.00    554438.73        10.47777         9.98300        25.72700        86.52700     25875.64

tx_kick_counts: 467145
tx_net_counts: 0
rx_kick_counts: 3707
rx_net_counts: 422234
tx_ratio: 0.0000
rx_ratio: 113.9018

467145,0,3707,422234,609989.94,25.72700

--- 256

============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec
----------------------------------------------------------------------------------------------------------------------------
Sets        61880.68          ---          ---        18.96687        17.66300        63.48700       286.71900      4761.75
Gets       616694.88         0.00    616694.88        18.79148        17.53500        56.06300       288.76700     24028.15
Waits           0.00          ---          ---             ---             ---             ---             ---          ---
Totals     678575.56         0.00    616694.88        18.80748        17.53500        56.83100       288.76700     28789.90

tx_kick_counts: 402152
tx_net_counts: 0
rx_kick_counts: 4704
rx_net_counts: 443469
tx_ratio: 0.0000
rx_ratio: 94.2749

402152,0,4704,443469,678575.56,56.83100

0.266144
0.342502
0.421676
0.491294


podman run -d --rm --network host --privileged scx_simple:0.0.1 /schedulers/scx_simple -f -p


响应度
- 对于实时任务： 即满足执行条件时，尽快执行(共置)

redis
- 低负载时，频繁epoll wait， 满足条件时，需要尽快唤醒，高HZ有利于快速响应
- 高负载时，几乎没有epoll wait， 表现为一个CPU+网络敏感应用，高HZ影响执行性能

多程序沙箱

安全serveless
- 内部混部
- 外部调整
对标pod

Control Zone对于ebpf的好处
- 缩小监测场景，简化ebpf逻辑
- 降低并发数量，减少采集开销
- 避免不同ebpf程序之间的干扰

打包应用和其调度器

混部场景

混部场景带来的挑战
- 软件环境复杂（软件本身，软件的运行环境）
- 硬件环境复杂（SMT、NUMA、Turbor）
- 软硬件环境变化
  - 多租户场景，软件本身迭代特性
  - 物理机，热插拔的硬件，不常见
  - 虚拟机，vCPU迁移

linux现有调度的缺陷
软件的不同需求
- 普适的解决方案，以吞吐量为目标，大量的启发式逻辑，优先级缺乏明确的语义(更多的CPU时间(BE)还是更快的CPU分配(LC)？)
- 高优先级应用对延迟的要求
  - 抢占模式与优先级割裂，任务抢占并不考虑优先级
- 硬件环境的复杂
  - 利用CPU拓扑进行负载均衡
  - CPU Turbo 与 AVX指令(现网问题)
- 软硬件环境变化
  - 新应用如何选择调度类，如何设置优先级？
  - 支持硬件热插拔，但是Core调度是编译时选项，不得不重启

国内外研究
- 怎么解决软件的不同需求？
  - 时间片粒度降低到延时范围内，微秒级调度
  - 隔离资源，CPU粒度的划分
  LC与BE
  - 负载感知，parties
  - 动态时间片，latency nice，
- 怎么适配不同的硬件？
  - 兄弟核运行感知，Elfen
  - AVX指令监听，vruntime补偿
- 怎样适应软硬件环境变化？
  - 用户态运行时
  - 暴漏内核调度接口
  - 可扩展的调度类

问题建模
- 将所需硬件资源、调度器、内核都打包到一起
- 即 虚拟机、内核、可变调度机制

一种运行时可变调度的沙箱机制

了解应用的不同需求
- 围绕虚拟机的可观测性系统
  - 虚拟机特殊性分析
    - 如何实现各个层次的分析？
      - 各种工具的使用
- 资源使用倾向与干扰敏感度分析

解决混部场景的调度需求

基于沙箱的虚拟机环境怎么解决混部场景的不同需求？
- 时间片粒度降低到延时范围内 -> 使用内核HZ与抢占模型解决 -> 响应度优先内核
  - BE -> 吞吐量优先内核
- 网络感知的BPF调度器

基于沙箱的虚拟机环境怎么解决混部场景不同的硬件？
- CPU感知BPF调度器

沙箱系统
- 轻量级：使用虚拟机是为了进行隔离，而不是设备模拟，同时需要减少虚拟化开销
- 可变调度机制与应用管理

未来展望
- 更多eBPF探测
- DevOps系统

刘宇航老师
现在做的结果有么，面向混合部署
很多调度器场景已经有了
复杂的事情简单化
工作的意义和效果有展示么


Motivation
- 混部应用繁多，需求各不相同，且动态变化
- 硬件特性丰富，虚拟机场景中，存在变化可能
- Linux调度器在调试、部署上十分困难，内核态开发限制了语义表达，无法灵活地对环境的变化做出响应

make O=output LLVM=1 CC=clang  headers_install INSTALL_HDR_PATH=header

meson setup build --reconfigure  -Dbuildtype=release -Dprefix=~/bin \
    -Dbpftool=$BPFTOOL/bpftool \
    -Dlibbpf_a=$BPFTOOL/libbpf/libbpf.a \
    -Dlibbpf_h=$BPFTOOL/libbpf/include \
    -Dkernel_headers=/home/fhl/Workplace/Linux/ext/include/uapi/linux


podman run -it --rm --privileged --network host --pid host -v $PWD/.controlzone/tools/:/tools -w /tools ict.acs.edu/scheduler/scx_simple:0.0.1 /bin/bash  


使用响应度优先内核(250hz -> 1000hz), 尾延迟下降 75%， bpf调度下延迟也下降 75%
- 为进一步降低尾延迟，使用更高的中断频率，同时为防止过高中断频率对系统的影响，使用集中式调度(调度核心)

在arm上部署
- 资源有限且异构，有利于BPF调度策略发挥
- BPF策略的可迁移性

BPF对现有框架的修改，内核模块则是增量的功能

资源隔离手段与任务调度机制割裂，难以高效利用资源

针对多线程应用不同线程的资源使用特性进行定制调度

插件化调度子系统图（拼图）

User Prog BPF Map  ---  BPF Policy   ---    BPF Map    ---   BPF Prog
                            |                                   |
                    Scheduling Subsystem                   Other Subsystem


租房补贴（3600 + 1500）
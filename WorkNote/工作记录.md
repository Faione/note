# collector源码

# 联调大车

# CLI开发

# 心跳状态检查


流程修改
   - 控制中心只管理 启动 与 关闭
      - 其他逻辑由策略完成
   - agent 进行过滤
      - 防止信息过多、
   - 集成前端ui，query到策略中

Agent设计
   - agent 监听多个topic 
     - 多个topic的生成、销毁对性能的影响
   - 系统组件心跳 
   - 轮询观察状态

流程扩充
   - 集群资源监控


metric数据结构定义
   - container
   - host
   - map进行可扩展

控制器与收集器定义
   - 提供基本函数

etcd持久化

jaeger-collector测试
   - 简单api接收器
   - 连接 agent -> collector


连接存在的问题
   - grpc验证问题，collector拒绝了agent的gpc请求，导致agent无法将span数据发送给collector

报错
```json
 {
    "level": "error",
    "ts": 1636541439.3878434,
    "caller": "grpc/reporter.go:74",
    "msg": "Could not send spans over gRPC",
    "error": "rpc error: code = Unavailable desc = last connection error:connection error: desc = \"transport: Error while dialing dial tcp 172.30.0.2:14250: connect: connection refused\"",
    "stacktrace": "github.com/jaegertracing/jaeger/cmd/agent/app/reporter/grpc.(*Reporter).send\n\t/go/src/github.com/jaegertracing/jaeger/cmd/agent/app/reporter/grpc/reporter.go:74\ngithub.com/jaegertracing/jaeger/cmd/agent/app/reporter/grpc.(*Reporter).EmitBatch\n\t/go/src/github.com/jaegertracing/jaeger/cmd/agent/app/reporter/grpc/reporter.go:53\ngithub.com/jaegertracing/jaeger/cmd/agent/app/reporter.(*MetricsReporter).EmitBatch\n\t/go/src/github.com/jaegertracing/jaeger/cmd/agent/app/reporter/metrics.go:85\ngithub.com/jaegertracing/jaeger/cmd/agent/app/reporter.(*ClientMetricsReporter).EmitBatch\n\t/go/src/github.com/jaegertracing/jaeger/cmd/agent/app/reporter/client_metrics.go:121\ngithub.com/jaegertracing/jaeger/thrift-gen/agent.(*agentProcessorEmitBatch).Process\n\t/go/src/github.com/jaegertracing/jaeger/thrift-gen/agent/agent.go:176\ngithub.com/jaegertracing/jaeger/thrift-gen/agent.(*AgentProcessor).Process\n\t/go/src/github.com/jaegertracing/jaeger/thrift-gen/agent/agent.go:123\ngithub.com/jaegertracing/jaeger/cmd/agent/app/processors.(*ThriftProcessor).processBuffer\n\t/go/src/github.com/jaegertracing/jaeger/cmd/agent/app/processors/thrift_processor.go:122\ngithub.com/jaegertracing/jaeger/cmd/agent/app/processors.NewThriftProcessor.func2\n\t/go/src/github.com/jaegertracing/jaeger/cmd/agent/app/processors/thrift_processor.go:87"
}
```
- 思路：证书问题
   - 按照 jaeger 官方提供的DockerFile构造 base_image
   - 编译 agent、collector，通过 base_image构建镜像
- 结果
   - 仍然存在验证问题
- 计划
   - 查看grpc机制  


define里面是agent提供的基础方法

注册
创建收集器

插件的功能去pkg中增加
   编解码span 数据（model.span）

对容器进行监控、控制（应用无感）
Host进行监控

agent启动容器
   - 提供用户模板文件，用户填写并提交给agent
   - 获得注册信息，注册guid
   - guid作为环境变量，放入容器


容器guid: 镜像名称 + 随机数

collect -> manager



jaeger collector 增加 container id 筛选
   - get中，做span数据的解析与分类
   - 提供对于单个通道的关闭

guid 与 trace数据绑定 ： 使用 guid 作为 service name
1. tracer构建时传入 service name 标识当前追踪的进程
   - examples\hotrod\cmd\frontend.go
2. 打包span时， span.Process 保存了 service name 这一数据
   - model\model.pb.go
3. 解析 span 数据， 取出 Process 字段，就能够得到 service name




创建 jaeger-env 容器
   - 应用与 jaeger-env 交互
      - jaeger-env 暴露固定端口
         - StartContainer提供参数 ports
      - 应用与 jaeger-env 使用同一网络   


agent plugin http writer写数据时
  - host:port/get/{guid}
agent 接收到span数据的时
  - 维护 map[guid] chan []byte




log信息缓冲，未消费的话，维护一个队列


Collector

1124
   - manager - agent 联调测试



dcoker container -> cgroup path

net: 发送速率


- agent提供 k8s、opent stack 



jaeger UI
   - search
      - 左边 
   - trace detail
      - 核心 ms 
   - DAG: 整体数据(次数、延迟)
      - 选好应用之后


docker 私有库搭建
 

测试一

   - 初始化
      - 绑定服务容器与干扰容器到同一核心
         - 脚本绑定   
      - CPU核心分配

   - 服务容器
      - CPU使用率正弦波形
 
   - CPU干扰容器
      - 始终占满给定的CPU资源

   - 实验场景
      - 1、服务容器正常使用时正弦
      - 2、无调控，服务容器、干扰容器同时开启，服务容器使用率平缓且较低，资源无法满足
      - 3、有调控, 服务容器、干扰容器同时开启，服务容器使用率正常，资源满足


1. 小车
 - 服务容器
   - CPU使用率正弦波形
 - CPU干扰容器
      - 始终占满给定的CPU资源

2. 初始化
      - 绑定服务容器与干扰容器到同一核心
         - 脚本绑定   
      - CPU核心分配

3. 服务容器正常使用时正弦

4. 无调控，服务容器、干扰容器同时开启，服务容器使用率平缓且较低，资源无法满足

5. 有调控, 服务容器、干扰容器同时开启，服务容器使用率正常，资源满足


agent接入增加版本信息


1207
   - 从span数据中计算端到端延迟
   - agent关闭逻辑，是否需要注销所有对象
   - 针对 centos 做 cgroup
   - 测试 cpu 核心调整
   - 完善 agent 退出
   - 处理 no such function guid


1209
web讨论
- 前端
- 启动 agent
- 镜像存储, Docker私有库


agent 注册时增加 tag, 区别自己与其他应用
agent 保持发送心跳, 更新自己状态

sponge 提供 list 查询

搭建 jaeger-query


前端
   - 完善查询界面
   - 增加界面说明, 关联trace与dag
   - 根据请求数量区分dag节点
   - 心跳信息中增加动态指标信息

使用教程
   - 安装、测试、Q&A

基本规范

解决 vscode remote ssh 错误
   - 无法建立 code server 问题
      - 端口转发情况下
调研linux用户组管理  


固化常用tag

调研openstack

HotROD
   - 描述: UI，四个微服务，两个存储后端 
   - frontend: 
      - 描述: 前端微服务为 Javascript UI 提供服务，并对其他三个微服务进行 RPC 调用
         - 用户点击4个地址中的任意一个, 向后端发送搭车请求
         - 后端向前端返回汽车的车牌号和预计到达的时间作为响应 
   - customer
      - 描述: 接收 frontend 的 get 请求, 并执行一次数据库查询, 检查当前用户
   - driver   
      - 描述: 接收 frontend 的 RPC 请求, 并执行一系列Redis请求, 请求存在失败的可能, 找到司机(可能多个)
   - route:
      - 描述: frontend 根据上述请求的结果, 向 route 发送 get 请求,  route 返回路径信息给 frontend, frontend 处理后, 返回给用户

使用GPU的容器问题
   - 动态链接库失效, 同节点上镜像失效
   - agent对用户异构需求的支持
      - 主流之外，还有FPGA, npu等

测试 demo 启动顺序
   - darknet 
   - camera catcher
   - ui(启动websocket, 消费图片)

端到端追踪页面

agent控制k8s

tag 图可视化
   - 分级tag


agent 注册打tag
   - 定义请求结构体
   - 定义返回结构体
   - 更新 rpc
   - 更新 create 方法
   - 更新返回

agent 支持 docker-compose 

标签区分所有guid区分, 标注不运行自动删除的对象
   - auto delete

span多层解析

分页，内容描述

auto refresh


sponge && agent 代码review




viper
读比特流, 解析并存到文件里


注册传入 []请求
成功: map[applicationkey] reponse
失败: nil

application key 作为 key携带返回结果
   启动成功携带各个结果，否则全部删除
   假定操作

注册完成之后不用 ready 状态 (pendding)
   inspect 之后修改为 ready
 
提供 stop 与 restart

定义状态变化: terminating ->


RPC方法重命名


启动应用跳转的连接
   - web url 提取

小车demo优化(推迟)
完善小车应用的展示需求
   - 小车自己的延迟

小车页面点击之后可以看到延迟, 策略执行

单独追踪某一帧
   图片抓取服务闭环
      图片抓取程序最终收到前端发送的span

云服务 trace 接入规约

架构图更新, agent 提供 jaeger env
   - jaeger client 与 agent 绑定
      - metric、log、trace -> ES



frp安全性
   - tls, srpc

agent 抓udp
   - 收取 span
   - sampling

redis json

agent功能优化
   - handler中进行超时处理


dependency + dag

traces

trace

返回 data + err

直接完成方法


nnmsg R&Q 模型
   - https://github.com/codypiersall/pynng/blob/master/examples/reqprep.py


实时变换的延迟曲线
帧数率

agent 适配yml文件

containers:
  agent:
    appkey: trace_jaeger_client
    parent: trace_jaeger_collector
    child: ""
    name: ""
    image: jaeger-env
    cid: ""
    guid: ""
    status: NotReady
    portmap:
      "80": "8080"
    envskey:
      SUPB_GUID: "123456"
    cmd:
    - /bin/sh
    - boot.sh
    - hello
  collector:
    appkey: trace_jaeger_collector
    parent: supb_agent
    child: trace_jaeger_client
    name: ""
    image: jaeger-collector
    cid: ""
    guid: ""
    status: NotReady
    portmap:
      "80": "8080"
    envskey:
      SUPB_GUID: "123456"
    cmd:
    - /bin/sh
    - boot.sh
    - hello


模拟服务应用、模拟干扰应用
小车服务、图像处理服务


span 与 trace 分开
   - active trace bug
性能影响
   - Default rate: 10
   - Normal rate: 8-9
   - Exp rate: 6-7


frontend
   使用返回的端口，不同的地址连

camera service 
  -> dark 返回一个端口


todo 使用同一个 sock

benchmark容器


containers:
  emu-service:
    appkey: "one-core-emu-service"
    parent: ""
    child: ""
    name: ""
    image: 39.101.140.145:5000/cpu-ctrl-core-1
    cid: ""
    guid: ""
    status: NotReady
    portmap:
    envskey:
      SUPB_GUID: ""
    cmd:
    - /bin/sh
    - boot.sh

containers:
  cpu-interference-service:
    appkey: "one-core-interference-service"
    parent: ""
    child: ""
    name: ""
    image: 39.101.140.145:5000/interference-cpu-core-1
    cid: ""
    guid: ""
    status: NotReady
    portmap:
    envskey:
      SUPB_GUID: ""
    cmd:
    - /bin/sh
    - boot.sh


ip在环境变量
生成tag: host + port 

电调
方向舵机
摄像头舵机-上
摄像头舵机-下




1] sync info: %v&{map[01ad28f000:2 01d3dba500:2] map[01ad28f000:{0-95 1} 01d3dba500:{0-95 1}]}  function=agent.heartbeat


空包打时间戳
- 探测延迟

发送图片时间戳差异大
接收图片时间戳差异不大
  - 调整buffer 



- 子系统代码量，语言
  - 代码量: -
  - 语言: go  
- 子系统功能列表

| 功能     |           | 说明                                             |
| :------- | :-------- | :----------------------------------------------- |
| 调控功能 | 容器级别  | 基于docker api的容器生命周期管理                 |
|          |           | 基于cgroup的容器CPU、内存、I/O资源管理           |
|          |           | 基于tc工具的上行网络带宽、延迟控制               |
| 监控功能 | Host级别  | Host主机的CPU、内存、网络、I/O资源监控           |
|          | 容器级别  | 基于docker api的容器CPU、内存、网络、I/O资源监控 |
|          |           | 基于docker api的容器运行log监控                  |
|          | 应用级别  | 基于opentracing/jaeger的链路延迟监控             |
| 系统功能 | 状态管理  | 容器运行状态的心跳监控                           |
|          | RPC       | 基于总线的远程调控方法调用                       |
|          | 接入/退出 | 动态地控制主机、容器等再系统中的接入/退出        |

边缘服务器

CICD
形式化验证

agent-2021
- 信息高铁测调节点代理研究，实现调控数据、日志、延迟时间等跨区域、跨平台、跨设备的执行与采集。
agent-2022
- 信息高铁测调节点代理迭代，控制上实现OpenStack虚拟机、K8s容器编排系统的接入, 监控上对资源、日志、追踪的数据格式进行统一化方便存储与检索, 同时优化节点代理, 增强鲁棒性、稳定性, 降低资源开销, 实现性能的提升
- 信息高铁测调节点代理迭代, 实现对虚机、集群的控制，着力端到端追踪功能完善，并对其他指标进行标准化，优化节点代理，增强稳定性，提高性能
- 技术路线
  - 节点代理已有功能能够对容器、节点进行的监控与控制，但为契合信息高铁的发展以及当前云计算技术发展环境，节点代理应当提供对于虚机及集群的控制能力，来增加调控的维度，参考cncf目录，目前可行的技术有openstack，kubernetes与kubevirt，其分别对应于虚拟机平台，集群容器编排与尚未容器化的虚拟机编排，而在监控上，当前基于jaeger的链路追踪系统仍有不足之处，而作为接替Opentelemetry则能够提供更长久的技术支持与功能覆盖
    - OpenStack始于2010年，是Rackspace和美国国家航空航天局的合作项目，它主要作为基础设施即服务（IaaS）部署在公用云和私有云中，提供虚拟服务器和其他资源给用户使用。OpenStack平台由相互关联的组件组成，控制着整个数据中心内不同的厂商的处理器、存储和网络资源的硬件池。用户可以通过基于网络的仪表盘、命令行工具或RESTful网络服务来管理。
    - KubeVirt 是 Kubernetes 的虚拟机管理插件。目的是为 Kubernetes 之上的虚拟化解决方案提供一个共同基础。KubeVirt 的核心是通过 Kubernetes 的自定义资源定义 API 添加额外的虚拟化资源类型（尤其是 VM 类型）来扩展 Kubernetes。通过使用这种机制，Kubernetes API 可用于管理这些 VM 资源以及 Kubernetes 提供的所有其他资源。
    - OpenTelemetry 是一组 API、SDK、工具和集成，旨在创建和管理遥测数据，例如跟踪、指标和日志。该项目提供了一个与供应商无关的实现，可以将其配置为将遥测数据发送到用户选择的后端
  - 本项目着力节点代理的优化与功能强化，为顶层设计提供更丰富的监控维度与调控方式，满足当前及未来的需求


每台node上都安装agent
agent相互通信
Pod网络

opentelemtry 和 opentracing 和 jaeger client的关系

[openshift](https://www.tutorialspoint.com/openshift/openshift_docker_and_kubernetes.htm)



Jaeger 多用户
- agent 性能问题
  - api server 过多
  - 改进: 由agent统一接收请求，并对接入单位进行rpc调用   

- 控制中心需要提供 从 guid 查找根 agent 的功能
- 总线需要提供 guid 路由功能

- agent 先启动，后注册，不进行yaml文件的解析，只提供远程的调用


- helm 配置，k8s yamls
  - agent执行一个远程的操作
    - helm查询
    - 启停

- manager collector
  - 与 jaeger 

- 容器控制
- helm启停

- agent合并总控数据格式

- otrace，epuf
  

- agent 开发
  - 完成container所有功能
  - 完成所有测试

- helm 调研
  - helm环境搭建
  - helm操作调研
  - helm接入方式设计


- 工作
  - 开发工作
    - agent v1 收尾
    - agent v2 开发
  - 调研工作
    - ray调研
      - 用户
      - 系统(ing)
  - tracing调研
    - 概述
    - sampling(ing)
  - helm调研(ing)
- 计划
  - 开发
    - 短期
      - agent v0.2 版本测试
      - helm接入
      - 相关Demo开发
    - 长期
      - kubevirt接入
      - 子豪师兄
      - 云os功能支撑
  - 学习
    - 学校课程学习
  - 研究
    - tracing相关，adaptive sampling
    - unikernel 与 kubevirt




1. 每周进度
2. 分享
   - 王蕊老师: Splunk 调研汇报
   - 方浩镭: Agent v0.2 设计
3. 测调系统后续开发讨论

- agent 抽象
  - agent core抽象

- 其他
  - 工作汇报

- agent暂时不考虑 collector的数据聚合
  - reporter


- fast2022

- splunk lightstep 追踪系统


- 3月
  - v2版本重构基本完成(100%)
    - 完成 v2 版本重构文档
    - 完成基本容器监控、调控功能
  - agent性能测试 （20%）
    - 空载: 2-3%
    - 高负载: -

- 4月
  - k8s agent core 实现(10%)
    - k8s manager 兼容 helm 与 yaml 的启动方式
    - log 监控基于 k8s log
    - metric 监控基于 kube-prometheus
  - agent 数据转发监听端口
    - agent提供数据转发功能，通过暴漏接口的方式
  - agent bench mark
  - agent 开发框架


- 定制 jaeger collector 测试
- epbf exporter
- vector docker
- python profile
- prometheus server http handler

- agent 分布式方案
  - 每台Node上运行一个Agent
  - 局域网中，存在一个"网关角色", 运行在网关上的agent就是agent master，其余agent 作为slave 
  - 职责区分
    - agent master 负责启动 prometheus server 与 jaeger collector
      - 两者会将采集的数据发送到控制中心，并能够被控制中心控制
      - 两者都需要在启动是进行配置
        - Prometheus: 需要配置 job 以进行 exporter 的发现
        - Jaeger collector: 需要配置guid
    - agent slave 负责启动 prometheus exporter 与 jaeger agent 
      - Prometheus Exporter
        - 需要同步 ip:port 给 agent master
      - Jaeger Agent
        - 需要同步从Master 同步 Jaeger Collector URL
  - 分布式实现方案
    - Prometheus
      - 方案一，agent slave push
        - agent slave 启动完成 exporter 之后，提交 job 给 agent master
          - job_name: guid + expotername
          - instance: ip:port
        - agent master 收到job之后，修改 prometheus server config ，然后重启prometheus server
          - agent master 需要开启监听端口
      - 方案二，agent master pull
        - agent 通过配置获得 agent slave，拉取对应job信息
          - agent slave 需要开启监听端口
        - 信息拉取完毕后，agent server 再重启 prometheus server
      - 方案三，thrid database
        - agent slave 启动 exporter 之后，提交信息到 database
        - agent master监听数据改动，重启prometheus server

- agent v 0.3
  - api server -> controller
  - agentcore -> service
  - manager/collector -> component
  - dirver -> utils


- agent提供服务，用类别进行单独区分，作为微服务
  - 先使用协程替代
- agent运行过程中传递一种上下文，使得agent功能可以在运行过程中变化
- 每个节点上都允许daemon server
  - 注册host
  - 提供 run agent api
  - 从控制中心接收 agent yml， 启动对应的服务
    - 运行过程中，允许重载agent功能

- python程序监控
  - 监控程序调用栈及其延时，将监控数据反馈给控制中心
  - 关联延时与代码行，能够将关键代码行信息反馈给控制中心


- 总线需求
  - ISubscribe bug
    - （待解决）
  - RPC框架需求
    - 提供类似于http路由的多路复用功能
      - 下版本提供
    - 更多的参数传递方式
      - 路径参数、query参数等
      - 考虑提供


# 4.22-组会-记录 

- 主持人：丁兆云
  - 记录人：方浩镭

- 下次主持人：常子豪 
  - 下次记录人：白冰怡 

## 过进度
 
- 子豪师兄
  - 介绍合作项目，为大规模分布式验证平台，分布式场景的安全增强，与当前项目结合性未知
  - 正在准备系统领域(osdi、sosp)的调研分享
- 丁兆云老师
  - 资源展示图优化，需要使用Canvas进行实现，类似于子豪师兄PPT中的示意图
    - 一个柱子对应一个机器，机器之中是应用(容器)
  - DAG图优化，按照Splunk中的图例
- 方浩镭
  - linkerd在项目中的使用
- 马英杰
  - 日志展示页面优化，日志详细部分显示太窄

## 分享 

- 丁兆云——前端分享(Vue与React)
  - PPT优化，结构，逻辑，图例


kubectl -n test-helm-social-network port-forward --address=0.0.0.0 svc/nginx-thrift 8080
./wrk -D exp -t 1 -c 1 -d 20 -L -s ./scripts/social-network/compose-post.lua http://172.16.31.39:8080/wrk2-api/post/compose -R 1
./wrk -D exp -t 1 -c 1 -d 20 -L -s ./scripts/social-network/read-home-timeline.lua http://172.16.31.39:8080/wrk2-api/home-timeline/read -R 1
./wrk -D exp -t 1 -c 1 -d 20 -L -s ./scripts/social-network/read-user-timeline.lua http://172.16.31.39:8080/wrk2-api/user-timeline/read -R 1




python3 scripts/init_social_graph.py --graph=<socfb-Reed98, ego-twitter, or soc-twitter-follows-mun>


- socialnetwork镜像替换

- agent 扩展方式
  - 源码修改
  - 方法注册
    - agent 提供注册接口与sdk
    - 用户通过sdk注册方法到agent，并于agent保持通信

- CProfile场景
  - 实现机制
    - 用户使用 gluenet profile 模块
      - 抓取数据，并发送到总线(直接/间接)
  - 数据规约
    - Profile数据解析


- todo 容器启动默认资源值设置

sudo mount -o remount,rw '/sys/fs/cgroup'
sudo  ln -s /sys/fs/cgroup/cpu,cpuacct /sys/fs/cgroup/cpuacct,cpu

ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCqSzy//X4r8StU+7m8lMh7WI6UViA32seUeyFulPh7AxRoOoItoOnkZ7bWM0iBuGwP9kWNHnK1rzSU4EBj7ZRzrGdHZ0ugDnUZoj+Ubrb+l350Eyr/l5IP9qtANT6hccDjeB7wg4PPvkiVE4Bc7SKezBibR2SpEqo9AjjomrwzpHJCN0mD3velPKWraQvN9KftbAfyajwjjGaviHXZv4I/mU+WQO9jotZ5L5ltY5bdMQVQqxE/Y1cz56qxU09gZ2kL1X4gZC9e3e3DjiQn0UHzBxxGXvg9YNDY5nyDanye34Q45wvvlFJMm2fWFvX+2IQ/+ftHXvue76ExHECf9xEVsJJQDBD6XhFhqMAkTMV/GF2cC/ewJrHv1YX2v+BQYcO+hpndXu8xCsbUtR/7KZKRUXa5BXHyj0baqCD4Ff18Roaf+4yxi0NIyKPloK3Dt/rEFhhLIp2zwjcoA30vRaC2v2ArSNW67yLZEsDYA51W18aAO63NMB5QHtNdpOy5zfc= fhl



152.136.134.100:10048/jaeger-collector-glunet:latest
152.136.134.100:10048/glue_node_exporter:v0.1
docker.io/google/cadvisor:latest
quay.io/coreos/etcd:v3.4.14
jaegertracing/jaeger-agent:latest
prom/prometheus:latest
timberio/vector:0.21.0-distroless-libc

面向云计算场景的一站式监控方案




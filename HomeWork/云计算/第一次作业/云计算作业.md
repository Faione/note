# 云计算作业
## 一、作业
- 要求

调研KVM下的VCPU调度算法（实际上就是Linux进程调度算法），与课堂讲授的Xen Credit算法进行比较，分析两种调度的异同及各自的优缺点，完成调研分析报告。

- 评分说明

根据作业完成质量评分，最高分5分，没提交得0分；本次作业的得分计算在平时作业的得分内。

- 提交方式

word或pdf，以附件形式提交，文件名为“姓名-学号”

## 二、资料收集
### (1) PPT学习
- 虚拟化场景下，OS的地位等同于进程，因而虚拟化程序应当拥有高于OS的权限等级，正如OS的权限高于应用进程，进程向OS请求硬件资源，而虚拟化场景下，OS向虚拟化程序请求硬件资源，虚拟化程序来保证硬件资源的分配和调度

- 指令集架构
   - 特权等级
      - 分段虚拟内存管理中，段号中有2位用以表示特权级，CPU会将这一数据保存在段寄存器中
      - 2位即存在4种特权级别，由3->0依次增加，区分在于指令所访问资源的敏感程度
      - [CPU特权级分析](https://blog.csdn.net/farmwang/article/details/50094959) 
   - 指令类别
      - 特权指令: 只能在最高特权级运行,否则会产生GPF中断
      - 敏感指令: 一部分是特权指令，一部分则可以不在最高特权级执行，而根据特权级，进行的操作纬度有所不同
      - 临界指令: 非特权指令的敏感指令，即可不在最高特权级执行的指令
      - 普通指令: 只能在非最高特权级外执行的指令
- 虚拟化类型
   - 全虚拟化
      - BT + DE：vmware, 动态翻译内核的二进制代码，使OS运行在ring1，同时，对于需要VMM介入的地方插入trap，让VMM接管敏感指令或特权指令的模拟执行
         - 对于特权指令，由于其在非特权级下执行会产生GPF，因而可以让VMM捕获此中断以进行模拟执行
         - 对于用户指令，直接执行即可
         - 对于敏感指令，则需要特殊处理
      - OS不需要主动做出任何改变，一切都由VMM完成  
   - 半虚拟化
      - 修改OS代码，让其运行在ring1
      - 将特权指令、敏感指令的相关操作转为对Hypervisor的HyperCall，Hypervisor检验HyperCall并代为执行，而OS无法正常执行任何特权指令
      - 对用户级软件而言是透明的 
      - XEN
         -  位于OS和硬件(ring 0)之间, 为ring 1的OS kernel提供虚拟化的硬件资源
         -  管理和分配这些资源，并确保上层虚拟机(域Domain)之间的隔离
         -  设置特权域(Domain 0)协助管理其他域
  
- VCPU调度算法
   - Credit算法
      - [Credit算法](https://blog.csdn.net/gavinwjin/article/details/5178745) 
      - [Xen虚拟CPU空闲调度算法](https://wenku.baidu.com/view/cf230066657d27284b73f242336c1eb91a3733a9?bfetype=new)
      - [Xen调度算法分析](https://www.cnblogs.com/linanwx/p/5358510.html)
      - I: VCPU构成的队列
         - VCPU拥有优先级、状态信息、所属Domain信息
            - Domain信息; (weight, cap), 由credit算法为每个Domain赋予
               - weight: 决定Domain之间各自占用CPU时间片的比例
               - cap: 决定Domain占用CPU时间的上限值
         - 优先级队列
            - under队列
            - over队列   
      - P: 算法核心
         - 初始状态
            - 所有vcpu均在under队列中
            - 所有domain的初始credit = weight
         - 算法
            - 只调度under队列的队首vcpu 
            - 每当vcpu被调度时，其credit值就减少(相同)
            - 若credit值为负数，则将此vcpu放入over队列
            - under队列空时
               - ?按比例为所有domain的credit加上初始weight值，并将vcpu加入under队列      
      - O: 输出一个VCPU 
      - 评价

### (1) 作业
#### KVM下的VCPU调度算法
KVM下，VCPU就是一个线程，实际采用的调度算法与Linux进程调度的算法相同

- O(N)调度器
   - [参考](https://cloud.tencent.com/developer/article/1603917) 
   - 核心
      - O(n)调度器采用一个runqueue运行队列来管理所有可运行的进程，在主调度schedule函数中会选择一个优先级最高，也就是时间片最大的进程来运行，同时也会对喜欢睡眠的进程做一些补偿，去增加此类进程的时间片。当runqueue运行队列中无进程可选择时，则会对系统中所有的进程进行一次重新计算时间片的操作，同时也会对剩余时间片的进程做一次补偿
   - 实现细节
      - 初始化
         - 调度器定义了一个runqueue队列，所有状态为Running的进程都会添加到该队列中
         - task_struct结构 9

         ```c
         struct task_struct {
         
             long counter; // 进程时间片，由 nice值确定
             long nice; // 进程的静态优先级 (-20 ~ +19), 越小优先级越高
             unsigned long policy;  // 调度策略 RR 或 FIFO
             int processor; // 当前进程运行在那个CPU上
         
             unsigned long cpus_runnable, cpus_allowed; // 正在运行的 cpu, 该进程可以运行的CPU
         }
         ```

      - 算法核心
         - 遍历runqueue中的每一个进程，找到优先级最高的一个进程执行
         - 每次中断，都会让当前运行的进程counter值减1
         - 当没有可以运行的进程时，则对所有进程重新初始化counter 

- O(1)调度器 
   - [参考](https://cloud.tencent.com/developer/article/1603981)
   - 实现细节
      - 初始化
         - 每个运行的CPU都维护一个runqueue，runqueue中维护active、expired两个队列
            - active: 所有运行的进程都在acive中
            - expired: 所有时间片用完的进程都在expired中
               - 进程都按照优先级排列，优先级相同的进程挂载在同等级的链表中
               - 提供bimap，记录哪一个优先级中有可运行的进程
      - 算法核心
         - 从runqueue的active队列中的bitmap找到一个下标，该下标指向一个优先级，并从改优先级中取出一个进程进行调度
         - 当active队列中无进程可以调度时，就交换active与expired的指针，继续进行调度

- CFS调度器
   - [参考](https://cloud.tencent.com/developer/article/1603845) 
   - 实现细节
      - 初始化
         - CFS中没有时间片的概念，取而代之的是weight
            - weight通过nice值来转化，优先级越高的的进程对应的weight值越大
            - 相邻的nice值相差10%的CPU时间 

         - 调度器通过计算进程的虚拟时间vruntime来选择调度的进程
            - $vruntime=\frac{wall\_time * NICE0\_WEIGHT}{weight}$ 
               - wall_time: 进程实际运行的时间
               - NICE0_WEIGHT: nice值为0时对应的weight

         - 进程的运行时间分配满足如下公式
            - $\frac{进程的weight}{总的可运行进程weight}$

         -  每个CPU上的CFS运行队列通过以vruntime为key的红黑树组织
            - 每个CPU上可以存在多个运行队列，以针对不同特征的进程

      - 算法核心
         - 调度器每次都选择红黑树最左边即vruntime最小的进程调度
            - 较少被调度到的进程 wall_time 小，更容易被调度，防止饥饿
            - 优先级高的进程，weight值大，也就更容易被调度
         - 进程执行的所分配的时间，之后调度器再选择进程调度   





#### 算法比较
- O(n)

时间复杂度问题，时间复杂度是O(n),当系统中的进程很少的时候性能还可以，但是当系统中的进程逐渐增多，选择下一个进程的时间则是逐渐增大。而且当系统中无可运行的进程时，重新初始化进程的时间片也是相当耗时，在系统中进程很多的情况系下。
SMP扩展问题。当需要picknext下一个进程时，需要对整个runqueue队列进行加锁的操作，spin_lock_irq(&runqueue_lock);当系统中进程数目比较多的时候，则在临界区的时间就比较长，导致其余的CPU自旋比较浪费
实时进程的运行效率问题，因为实时进程和普通进程在一个列表中，每次查实时进程时，都需要全部扫描整个列表，导致实时进程不是很“实时”
CPU资源浪费问题：因为系统中只有一个runqueue,则当运行队列中的进程少于CPU的个数时，其余的CPU则几乎是idle状态，浪费资源
cache缓存问题：当系统中的进程逐渐减少时，原先在CPU1上运行的进程，不得不在CPU2上运行，导致在CPU2上运行时，cacheline则几乎是空白的，影响效率。
总之O(n)调度器有很多问题，不过有问题肯定要解决的。所以在Linux2.6引入了O(1)的调度器

- O(1)

O(1)调度器的引入主要是为了解决O(n)调度器的不足
O(1)调度器在赏罚机制上比O(n)调度器考虑的因素比较多，不再时像O(1)那样直接考时间片的大小来调度
但是O(n)和O(1)调度算法上核心还是通过判断一个进程的行为，比如爱睡眠来进程赏罚机制，爱睡眠来增大优先级，增大时间片的机制来获取更多的运行时间。
如果去看O(1)调度器的实现，没有O(n)算法那么简单明了，O(1)中加了需要时间的判断，各种情况的考虑，导致代码的阅读性很差，读起来很费劲。
当然了时代还是要前进的，O(n)和O(1)调度器是为CFS调度器出现地提供了很好的环境

- CFS
在O(n)和O(1)调度器中都是通过nice值来分配固定的时间片，CFS中没有时间片的概念
CFS调度器中通过进程的静态优先级来计算进程的权重，进程的权重就代表了此进程需要获取的CPU的时间比例
通过进程的weight和进程的实际运行时间来计算进程的vruntime虚拟时间。
当进程加入到运行队列，调度器会时刻来更新进程的vruntime，来达到公平
调度器每次调度的时候只选择运行队列中虚拟时间最小的进程，当此进程运行一段时间后，vruntime就会变大
这时候就需要调度时候就需要重新选择新的最小vruntime的进程来执行，上次被调度出去的进程则就需要根据vrumtime的值来选择自己在运行队列的位置 

- credit

#### 总结

 	 
# 基于Prometheus的Cilium监控

- [基于Prometheus的Cilium监控](#基于prometheus的cilium监控)
  - [环境准备](#环境准备)
  - [监控配置](#监控配置)
  - [监控指标](#监控指标)
    - [Cilium指标](#cilium指标)
      - [service map相关指标](#service-map相关指标)
      - [统计相关指标](#统计相关指标)
    - [Istio指标](#istio指标)
    - [Linkerd指标](#linkerd指标)
  - [service map数据结构](#service-map数据结构)
  - [L7监控方案](#l7监控方案)
## 环境准备

- 安装脚本
  - `--set hubble.metrics.enabled=""` 来关闭metric
  - `--reuse-values`可以复用已有的value, 搭配`helm upgrade`已避免覆盖先前配置
  - `HUBBLE_METRIC_CONFIG`用来设置需要从hubble中获取的监控指标以及其配置
    - [hubble监控设置](https://docs.cilium.io/en/stable/operations/metrics/)

**更改现有配置**

```shell
RELEASE="cilium"
CHART="cilium/cilium"
VERSION="1.12.1"
NAMESPACE="kube-system"

HUBBLE_METRIC_CONFIG="{flow:destinationContext=pod;sourceContext=pod,http:destinationContext=pod;sourceContext=pod}"

helm upgrade $RELEASE $CHART --version $VERSION \
  --namespace $NAMESPACE \
  --reuse-values \
  --set hubble.relay.enabled=true \
  --set hubble.metrics.enabled=$HUBBLE_METRIC_CONFIG \
  --set hubble.ui.enabled=true
```


**从零开始**

```shell
REPO_NAME="cilium"
REPO_URL="https://helm.cilium.io/"

RELEASE="cilium"
CHART="cilium/cilium"
VERSION="1.12.1"
NAMESPACE="kube-system"

HUBBLE_METRIC_CONFIG="{flow:destinationContext=pod;sourceContext=pod,http:destinationContext=pod;sourceContext=pod}"


helm repo add $REPO_NAME $REPO_URL

helm install $RELEASE $CHART --version $VERSION \
  --namespace $NAMESPACE \
  --set hubble.relay.enabled=true \
  --set hubble.metrics.enabled=$HUBBLE_METRIC_CONFIG \
  --set hubble.ui.enabled=true 
```

hubble本身cilium隔离，当hubble先于cilium启动时，cilium无法获得其信息导致不能管理，因此需要严格两者的启动顺序，或在cilium启动之后，将未被管理的部分纳入其中(重新pod，让cilium感知)

```shell
$ kubectl get pods --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,HOSTNETWORK:.spec.hostNetwork --no-headers=true | grep '<none>' | awk '{print "-n "$1" "$2}' | xargs -L 1 -r kubectl delete pod
```

**prometheus**

部署prometheus

```shell
$ kubectl apply -f <slim-prometheus-yaml>
```

查看prometheus

```shell
$ kubectl -n cilium-monitoring port-forward service/prometheus --address 0.0.0.0 3000:9090
```

## 监控配置

hubble meric配置`flow:destinationContext=pod;sourceContext=pod`，使得每一条flow的metric，都会按`namespace/podname`的格式，增加`destination` 与 `source` label，即这条流产生的源节点与目的节点，通过这些信息，能够不断聚合得到关系图谱
- `protocol`标识了flow的协议, 如HTTP,TCP,UDP等
- `verdict`标识了cilium对flow的处理，DROPPED表示丢弃，FORWARDED表示让行
- `type`标识了flow的层级

## 监控指标

### Cilium指标

#### service map相关指标

**hubble_flows_processed_total**

hubble_flows_processed_total对边有基础的描述

```
hubble_flows_processed_total{
    source="default/tiefighter",
    destination="default/deathstar-6c94dcc57b-2chb9",
    type="L7",
    protocol="HTTP",
    subtype="HTTP",
    verdict="DROPPED",  

    app_kubernetes_io_managed_by="Helm",
    instance="192.168.50.139:9965",
    job="kubernetes-endpoints",
    k8s_app="hubble",
    namespace="kube-system",
    service="hubble-metrics"
    } : 2
```

**hubble_http_requests_total**

hubble_http_requests_total描述http请求

```
hubble_http_requests_total{
    source="default/tiefighter",
    destination="default/deathstar-6c94dcc57b-2chb9",
    protocol="HTTP/1.1",
    method="POST",  

    app_kubernetes_io_managed_by="Helm",
    instance="192.168.50.139:9965",
    job="kubernetes-endpoints",
    k8s_app="hubble",
    namespace="kube-system",
    service="hubble-metrics"
    } : 1
```
**hubble_http_responses_total**

hubble_http_requests_total描述http响应
- 状态码可用于统计

```
hubble_http_responses_total{
    source="default/deathstar-6c94dcc57b-2chb9",
    destination="default/tiefighter",
    method="POST",
    status="200",   

    app_kubernetes_io_managed_by="Helm",
    instance="192.168.50.139:9965",
    job="kubernetes-endpoints",
    k8s_app="hubble",
    namespace="kube-system",
    service="hubble-metrics"
    } : 4
```


#### 统计相关指标

**hubble_http_request_duration_seconds_bucket**

http请求时间的频数分布直方图数据
- `le`: less than or equal, `le=0.005`即频数区间`[0, 0.005)`, 即延迟在 0.005 秒以内的请求数量
- 使用 histogram_quantile(0.5, prometheus_http_request_duration_seconds_bucket) 计算50分位尾延迟

```
hubble_http_request_duration_seconds_bucket{
    source="test-guid-0/deathstar-6c94dcc57b-jgszb",
    destination="test-guid-0/tiefighter",
    le="0.005",
    method="POST",
    
    app_kubernetes_io_managed_by="Helm",
    k8s_app="hubble",
    instance="192.168.50.192:9965",
    job="kubernetes-endpoints",
    namespace="kube-system",
    service="hubble-metrics"
    } : 14
```

**hubble_http_request_duration_seconds_sum**

http请求的总时间
- 结合 http请求的总次数 可计算平均请求时间

```
hubble_http_request_duration_seconds_sum{
    source="test-guid-0/deathstar-6c94dcc57b-jgszb",
    destination="test-guid-0/tiefighter",
    method="POST",

    instance="192.168.50.192:9965",
    job="kubernetes-endpoints",
    k8s_app="hubble",
    app_kubernetes_io_managed_by="Helm",
    namespace="kube-system",
    service="hubble-metrics"
    }
```

**hubble_http_request_duration_seconds_count**

http请求的总次数

```
hubble_http_request_duration_seconds_count{
    source="test-guid-0/deathstar-6c94dcc57b-jgszb",    
    destination="test-guid-0/tiefighter",
    method="POST",

    namespace="kube-system",
    service="hubble-metrics",
    instance="192.168.50.192:9965",
    job="kubernetes-endpoints",
    k8s_app="hubble",
    app_kubernetes_io_managed_by="Helm"
    } : 14
```

### Istio指标

istio_requests_total

```
istio_requests_total{
    app="details",
    connection_security_policy="mutual_tls", 
    destination_app="details", 
    destination_canonical_revision="v1", 
    destination_canonical_service="details", 
    destination_cluster="Kubernetes", 
    destination_principal="spiffe://cluster.local/ns/default/sa/bookinfo-details", 
    destination_service="details.default.svc.cluster.local", 
    destination_service_name="details", 
    destination_service_namespace="default", 
    destination_version="v1", 
    destination_workload="details-v1", 
    destination_workload_namespace="default", 
    instance="172.17.0.6:15020", 
    job="kubernetes-pods", 
    namespace="default", 
    pod="details-v1-7f4669bdd9-dg7vn", 
    pod_template_hash="7f4669bdd9", 
    reporter="destination", 
    request_protocol="http", 
    response_code="200", 
    response_flags="-", 
    security_istio_io_tlsMode="istio", 
    service_istio_io_canonical_name="details", 
    service_istio_io_canonical_revision="v1", 
    source_app="productpage", 
    source_canonical_revision="v1", 
    source_canonical_service="productpage", 
    source_cluster="Kubernetes", 
    source_principal="spiffe://cluster.local/ns/default/sa/bookinfo-productpage", 
    source_version="v1", 
    source_workload="productpage-v1", 
    source_workload_namespace="default", 
    version="v1"
    } : 36
```

### Linkerd指标

**tcp_open_total**

```
tcp_open_total{
    app="emoji-svc",
    client_id="web.emojivoto.serviceaccount.identity.linkerd.cluster.local",
    control_plane_ns="linkerd",
    deployment="emoji",
    direction="inbound",
    instance="172.17.0.10:4191",
    job="linkerd-proxy",
    namespace="emojivoto",
    peer="src",
    pod="emoji-55c59cf485-rkszq",
    pod_template_hash="55c59cf485",
    srv_kind="default",
    srv_name="all-unauthenticated",
    target_addr="172.17.0.10:8080",
    target_ip="172.17.0.10",
    target_port="8080",
    tls="true",
    version="v11",
    workload_ns="emojivoto"
    } : 1
```

**http_client_requests_total**

```
http_client_requests_total{
    client="k8s",
    code="200",
    component="destination",
    instance="172.17.0.4:9996",
    job="linkerd-controller",
    method="get"
    } : 1
```

## service map数据结构

**hubble ui 数据结构**

hubble ui 源码中的mock数据

```
# HubbleLink(edge)
[
  {
    id: 'reserved:world:outgoing',
    sourceId: 'reserved:world:outgoing',
    destinationId: 'a8de92d55119c9a6bb6a6dd66bcf012fabefb32d',
    destinationPort: 443,
    ipProtocol: IPProtocol.TCP,
    verdict: Verdict.Forwarded,
  },
  ...
]

# HubbleService(node)
[
  {
    id: 'reserved:world:outgoing',
    name: 'World',
    namespace: selectedNamespace,
    labels: [{ key: 'reserved:world', value: '' }],
    dnsNames: [],
    egressPolicyEnforced: false,
    ingressPolicyEnforced: false,
    visibilityPolicyStatus: '?unknown?',
    creationTimestamp: dataHelpers.msToPbTimestamp(Date.now()),
  },
  ...
]
```

**旧gluenet ui 数据结构**

基于jaeger trace的数据

```
# Links
"dag_links": [
    {
        "source": "nginx-thrift",
        "target": "home-timeline-service",
        "traceIds": [
            "00736683e0e57ee0"
        ],
        "value": 4
    },
    ...
]

# Nodes(not array)
"dag_nodes": {
    "compose-post-service": {
        "agent_guid": "64809fc89e07746706983c09d891cf21",
        "guid": "f18af130113b0db7554d46d9a994b873",
        "id": "docker://526d583390d50a5b9fbc5b4dc8f81bd475d3f65789c0f6b8c0d54d792459f00d",
        "image": "yg397/social-network-microservices:latest",
        "name": "compose-post-service",
        "platform": "kubernetes",
        "pod": "compose-post-service-6d9984c876-6cl6f"
    },
    ...
}
```

**hubble observer**

gRpc请求hubble获得的数据(以json编码输出)
- Summary中为总结性的内容, 包括协议，以及协议的详细信息
- time记录的是实际flow产生的时间

TCP flow

```json
{
    "flow": {
        "time": "2022-09-22T02:45:37.120249940Z",
        "verdict": "FORWARDED",
        "ethernet": {
            "source": "86:eb:f5:0d:13:13",
            "destination": "32:57:95:25:31:5d"
        },
        "IP": {
            "source": "10.0.0.104",
            "destination": "10.0.0.240",
            "ipVersion": "IPv4"
        },
        "l4": {
            "TCP": {
                "source_port": 8181,
                "destination_port": 50810,
                "flags": {
                    "PSH": true,
                    "ACK": true
                }
            }
        },
        "source": {
            "ID": 357,
            "identity": 29648,
            "namespace": "cilium-test",
            "labels": [
                "k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=cilium-test",
                "k8s:io.cilium.k8s.policy.cluster=default",
                "k8s:io.cilium.k8s.policy.serviceaccount=echo-other-node",
                "k8s:io.kubernetes.pod.namespace=cilium-test",
                "k8s:kind=echo",
                "k8s:name=echo-other-node"
            ],
            "pod_name": "echo-other-node-6d8549cc6c-wxdvd",
            "workloads": [
                {
                    "name": "echo-other-node-6d8549cc6c",
                    "kind": "ReplicaSet"
                }
            ]
        },
        "destination": {
            "identity": 1,
            "labels": [
                "reserved:host",
                "reserved:kube-apiserver"
            ]
        },
        "Type": "L3_L4",
        "node_name": "cilium",
        "reply": true,
        "event_type": {
            "type": 4,
            "sub_type": 3
        },
        "traffic_direction": "INGRESS",
        "trace_observation_point": "TO_STACK",
        "is_reply": true,
        "Summary": "TCP Flags: ACK, PSH"
    },
    "node_name": "cilium",
    "time": "2022-09-22T02:45:37.120249940Z"
}
```


Http flow

```json
{
    "flow": {
        "time": "2022-09-22T07:15:13.011142862Z",
        "verdict": "FORWARDED",
        "IP": {
            "source": "10.0.0.10",
            "destination": "10.0.1.61",
            "ipVersion": "IPv4"
        },
        "l4": {
            "TCP": {
                "source_port": 80,
                "destination_port": 51794
            }
        },
        "source": {
            "ID": 235,
            "identity": 4051,
            "namespace": "default",
            "labels": [
                "k8s:app.kubernetes.io/name=deathstar",
                "k8s:class=deathstar",
                "k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=default",
                "k8s:io.cilium.k8s.policy.cluster=default",
                "k8s:io.cilium.k8s.policy.serviceaccount=default",
                "k8s:io.kubernetes.pod.namespace=default",
                "k8s:org=empire"
            ],
            "pod_name": "deathstar-6c94dcc57b-hljrj"
        },
        "destination": {
            "identity": 13798,
            "namespace": "default",
            "labels": [
                "k8s:app.kubernetes.io/name=tiefighter",
                "k8s:class=tiefighter",
                "k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=default",
                "k8s:io.cilium.k8s.policy.cluster=default",
                "k8s:io.cilium.k8s.policy.serviceaccount=default",
                "k8s:io.kubernetes.pod.namespace=default",
                "k8s:org=empire"
            ],
            "pod_name": "tiefighter"
        },
        "Type": "L7",
        "node_name": "cilium",
        "l7": {
            "type": "RESPONSE",
            "latency_ns": "10246020",
            "http": {
                "code": 200,
                "method": "POST",
                "url": "http: //deathstar.default.svc.cluster.local/v1/request-landing",
                "protocol": "HTTP/1.1",
                "headers": [
                    {
                        "key": "Content-Length",
                        "value": "12"
                    },
                    {
                        "key": "Content-Type",
                        "value": "text/plain"
                    },
                    {
                        "key": "Date",
                        "value": "Thu, 22 Sep 2022 07:15:13 GMT"
                    },
                    {
                        "key": "X-Envoy-Upstream-Service-Time",
                        "value": "6"
                    },
                    {
                        "key": "X-Request-Id",
                        "value": "58d8fc20-a7d3-40f3-bf2d-c853078e5972"
                    }
                ]
            }
        },
        "reply": true,
        "event_type": {
            "type": 129
        },
        "traffic_direction": "INGRESS",
        "is_reply": true,
        "Summary": "HTTP/1.1 200 10ms (POST http://deathstar.default.svc.cluster.local/v1/request-landing)"
    },
    "node_name": "cilium",
    "time": "2022-09-22T07:15:13.011142862Z"
}
```


## L7监控方案


用户定义L7监控的目标
- 选择pod， 并为改pod添加Egress，Ingress两类标签，标签数目不限
  - Egress类: 目标端口, 目标协议(SUB/POC)
  - Ingress类: 服务端口, 服务协议(SUB/POC)
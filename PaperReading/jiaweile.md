## 阅读记录 

### Problem & Background


#### Background

**ab initio molecular dynamics(AIMD)**

- 分子动力学MD
  - 分子动力学的准确性取决于对于原子之间相互关系的描述

- 从头算动力学AIMD
  - 利用密度泛函理论(DFT)等第一性原理电子结构方法演化原子系统和原子间力
  - 允许化学键的裂解和形成事件的发生，并解释了电子极化效应
  - 基于 DFT 的可信度，AIMD一直是微观理解广泛问题的主要途径，如药物发现、复杂的化学过程、纳米技术等。

- AIMD与高性能计算
  - AIMD的计算代价通常与电子自由度的数量的立方体成比
  - 2006-2019，超算算力提升了550倍，但能够模拟的系统只提升了8倍(cubic-scaling law)
  - DFT方法一直在积极发展中，但复杂性的前因子仍然很大，在MD模拟中可达到的时间尺度仍然相当短

**Deep Potential Molecular Dynamics**

- 基于机器学习的MD(MLMD)提供了一种加速AIMD的新范式
  - 其中一种模型Deep Potential(DP), 已经证明了能够达到与AIMD相当的精度，并且效率接近于EFF-based MD，并且在一些场景中已经有所使用
  - 能够解决一些对于AIMD来说有挑战性的问题


<!-- For me -->

- 嵌入矩阵
  - 稀疏矩阵相比于列表有更好的计算性能，但是过于稀疏时，会过度占用资源
    - 如 独热码 编码的文本于直接用 序号列表 编码的文本
  - 通过 Embedding 层，对原本的稀疏矩阵进行降为

#### Problem

- 常用MD领域中，对于系统规模的要求从数千到数亿个原子
  - 其中一些问题需要时间尺度扩展到微秒及更长，这远远超出了AIMD的范围
  - 另一些问题则需要比DFT所能提供的更高的准确性(化学准确性)，需要使用CCSD(T)
    - CCSD(T)的计算复杂度是系统规模的7次幂

- 现有特殊的模拟技术来增强慢过程的采样来处理这种情况
  - 需要相对较长的时间尺度的数十或数百纳秒的MD模拟

- 基于经验力场(EFF)的MD方案，可以很容易地扩展到数百万个，甚至数万亿个原子，但准确性经常受到质疑
  - 在涉及多种元素或键的形成和解理的情况下开发EFFs一直具有挑战性，而且对于许多实际问题，没有合适的EFFs
- 能够模拟化学反应的反作用力场，如戈达德和合作者引入的REAXFF方法
  - 然而，这些方法缺乏DFT的通用性和预测能力
- MD社区迫切需要从根本上提高AIMD的效率，同时保持其准确性

### Challenges

- 对传统的 DFT-based AIMD 体系来说，克服算力限制是一个挑战，即使使用最快的HPC(高性能计算集群)


### State-of-the-arts

- 分子模拟的一个重要目标是用从头计算的精度来模拟涉及数亿个原子的现实过程
  - 为了实现这一目标，人们已经做出了重大努力，在不损失准确性的情况下提高AIMD
  - 但是通过粗略估计，假设未来的HPC性能将继续以与过去14年相同的速度提高，这将需要几十年才能在传统AIMD技术上进行目标规模和时间尺度的模拟

- MLMD提供了一种不会损失进度而绕过传统AIMD的机会，然而当前社区的主要方向仍然是致力于继续提高机器学习的可表征性与可转移性，并且只在一些不需要特别大规模的MD模拟的地方使用机器学习来解决问题
  - 当前实现的MLMD基本面向桌面GPU或者CPU-only集群中，它们都不能充分利用现代异构超级计算机上的加速器所提供的计算能力


- DeePMD是一个开源的项目，基于 LAMMPS MD 平台 与 TensorFlow 深度学习平台，具有AIMD计算精度的大规模DeePMD模拟仅从概念上被证明是可能的，但从算法和实现的角度来看，为现代异构HPCs优化的代码尚未有可用的实现
  - DeePMD 的关键功能是实现了基于DP模型来预测原子能和力的计算
  - 现有的DeePMD-kit实现并不能充分利用像Summit这样的现代异构超级计算机的计算能力, 这是因为
    - 程序库基于有一个GPU或多个CPU的单节点进行设计
    - 为环境矩阵、力和维里值引入的定制TensorFlow操作仅在多CPU上实现
    - DP模型所使用的DNN的大小通常小于模式检测和语言处理等普通深度学习应用程序中所采用的大小，这意味着在DP应用程序中，计算密集型操作的每个单独步骤也更小
  - 并且, 内存带宽与延迟是提升DeePMD-kit包计算效率的阻碍

> LAMMPS MD: 
>> 包含原子级别的信息与整合的运动方程
> TensorFlow: 
>> 使用TensorFlow的标准算子实现计算中所要使用的模型
>> 提供了GPU操作的支持，通过链接TensorFlow库，能够轻易的对DeePMD kit 进行加速

- [机器学习Kernel](https://blog.csdn.net/Y_hero/article/details/88720105)
### Key insights/ideas/techniques

- 通过结合DP前所未有的表现能力与在异构GPU体系上高伸缩与调整的实现，来提供一种高效和高度可伸缩的方法来执行AIMD精度的MD模拟

#### 算法创新

- 主要创新
  - 通过引入一种新的邻域列表的数据布局来增加DeePMD的计算粒度，以避免在嵌入矩阵的计算中出现分支
  - 邻域列表的新数据结构中的元素被压缩为64位整数，以更有效地实现定制的TensorFlow operators的GPU优化
  - 开发了DP模型的混合精度计算。计算密集型任务以单精度或半精度执行，而不降低物理观测的精度

**增加计算粒度**

- DP模型中计算最密集的部分是嵌入矩阵，计算模式由记录在邻居列表中的邻居的顺序定义

- 由于描述符是排列不变的，具有不同顺序的邻域列表在准确性方面是等价的
  - 利用这一观察结果，重新设计了邻居列表的数据布局，根据邻居的类型对其进行排序，并且，在每种类型中，我们根据它们的相对距离对邻居进行排序
  - 同一类型的邻居被填充到与该类型对应的邻居的截止数

- 效果
  - 第一次排序（根据邻居类型）和填充将具有相同类型的邻居对齐，因此避免了在嵌入矩阵计算中根据邻居的类型进行的条件分支
    - 从而大大增加了计算粒度，是利用gpu提供的计算能力的关键部分
  - 第二次排序总是根据邻居与中心原子的距离来选择列表中的邻居
    - 这样，如果邻居的数量偶尔波动超过Nmax中定义的Nmax，则只保留Nmax的最近邻信息，避免了忽略近邻时发生的非物理现象

**定制TensorFlow操作数的优化**

- 由于内存合并的问题，邻居列表的 array of structures 数据布局使得无法在GPU上实现有效的内存访问, 常规的做法是将 array of structures 转化为 structure of array

- 然而，在DeePMD-kit中，我们提出了一种更有效的存储邻居列表的方法，将邻居列表中的每个元素压缩为一个64位无符号整数
  - 64位无符号整数的20位被分成3个部分，以存储邻居列表中的一个元素：4位表示原子类型，10位表示原子距离，6位表示原子索引
    - 这些位的选择经过了充分的考量，并且在一般的 DeepMD 模拟中很少会有越界
  - 等式: 
  $$\alpha(j) \times 10^{16} + \left[\lvert r_{ij}  \rvert \times 10^{6}\right] + j$$

- 排序前的压缩和排序后的解压缩都可以通过CUDA定制的核(一种运算)来加速
- 对压缩后的邻居列表进行排序可以使比较次数减少一半，而不影响算法的准确性，并通过调用NVIDIA CUB库来进行
  - 该库为CUDA编程模型的每个层提供了最先进的和可重用的软件组件，包括全块排序

- 在我们的实现中，所有定制的TensorFlow操作，环境，ProdForce 与 ProdViral，分别用来计算环境矩阵、力和viral方程 都在GPU上进行了迁移和优化
  - 特别是，我们利用了一个细粒度的并行性来利用GPU的计算能力

- 以上设计让所有计算密集型任务在GPU上进行，之后，我们通过在初始化阶段分配一个主干的GPU内存，并在整个MD模拟过程中重新使用GPU内存，进一步减少了GPU内存分配的时间。CPU-GPU内存复制操作也进行了优化，以消除非必要的数据传输过程

**混合精度计算**

- 基于DNN的DP模型的近似特性为使用混合精度计算提供了机会

#### 神经网络创新

- 未被优化的标准TensorFlow运算是剩余的计算开销
  - 浮点运算主要是 MATMUL(矩阵乘法) 与 TANH (激活函数) 
  - 其他运算如 CONCAT(矩阵连接) 与 SUM (矩阵加法) ，对带宽敏感但浮点运算开销小

- DeePMD-kit 中多数运算都涉及高矩阵与瘦矩阵的矩阵乘法，这导致 SUM 操作开销很大，且标准的TensorFlow运算没有对这种情况进行优化，为此我们重新设计了TensorFlow执行图中的几种计算

**使用GEMM替代 MATMUL 与 SUM**

- 常规TensorFlow计算图中, $ x \cdot W + b$  运算使用 MATMUL 与 SUM 两种运算符实现，对于常规的数据驱动应用，矩阵 x 与 W 规模足够大以至于相比之下SUM的开销可以忽略，但在DeePMD中，由于矩阵是高矩阵，这使得 SUM 操作的开销必须重视
  - 在对计算图的优化中，使用一个 CUBLAS GEMM 调用替代 MATMUL 与 SUM 操作

**使用GEMM替代 CONCAT 与 SUM**

- 常规TensorFlow计算图中, $ \left( x, x \right) + ...$ 运算使用 CONCAT 运算符与 SUM 运算符实现
  - 对这样的运算的优化，使用矩阵乘法替代CONCAT运算，即 $ \left( x, x \right) \rightarrow  x \times  \left( I, I \right)$， 并同样地使用 CUBLAS GEMM 调用替代转化之后的 MATMUL 与 SUM 
    - 矩阵乘法的转化只比 CONCAT 略快，主要的性能提升来自于对 SUM 运算的优化

**对于TANH 与 TANHGrad 的 CUDA 核聚合**

- TANH 是激活函数，TANHGrad是反向传播中的求导函数
- 对于 $tanh(x)$ 的求导依然是关于 $tanh(x)$ 的函数，即 $\Delta tanh(x) = 1 - tanh^2(x)$
  - 对 DeePMD-kit 的优化中，为节省计算事件，在一个CUDA定制核中实现了 TANH 与 TANHGrad 运算
    - 由于TANHGrad的GPU内存是在正向传播中分配的，所以这种优化本质上是用时间交换空间

**减少MPI通信瓶颈**

- 对于DP可以采用与LAMMPS类似的并行化方案, 即将 LAMMPS 中实现的 EFFs 替换为 DP，同时，LAMMPS也被用于维护系统的空间划分和子区域之间的所有通信
- 每次 DeePMD 步骤中有两种主要的MPI通信
  - 相邻MPI任务之间的ghost区域的通信
  - 对于物理特性的全局还原
    - 尽管这些物理特性只有一个双精度数，相应的MPIAlldeses操作以延迟占主导，但在极大规模计算中，优化后的DeePMD-kit的缩放受到隐式MPI障碍的阻碍
- 实现中对于MPI的通信优化
  - 使用 CUDA-aware IBM Spectrum MPI 优化ghost区域的通信
  - 将输出频率降低到每20步一次，同时，使用MPI_Iallreduce 代替 MPI_Allreduce，以进一步避免隐式MPI障碍

### Lessons learned from experiments